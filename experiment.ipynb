{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd286c88-ce33-4be7-8aec-3c3fe5176c40",
   "metadata": {},
   "source": [
    "# Training a Lipschitz constrained model\n",
    "\n",
    "This notebook has two settings to choose from:\n",
    "1. MLP on CIFAR-10\n",
    "2. Transformer on Shakespeare text\n",
    "\n",
    "Within these configs, you can set the optimizer (AdamW, Muon), the weight norm constraint method (none, spectral capping, spectral normalization), and other hyperparameters.\n",
    "\n",
    "To train a 145M parameter transformer, check out the `/nanogpt` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842cc090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laker/lipschitz-transformers/lipschitz/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "from configs import parse_config_from_json\n",
    "from data_loaders import get_data_loader\n",
    "from models import create_model\n",
    "from optimizers import get_optimizer\n",
    "from trainer import Trainer\n",
    "from utils import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87ea7a",
   "metadata": {},
   "source": [
    "Specify the training setup. All the options are available in `configs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ea2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_mlp_muon_constrained = {\n",
    "    'optimizer': 'muon',  # or adam\n",
    "    'project': {'default': 'soft_cap'},  # specify per-layer (none, soft_cap, spec_normalize, etc.)\n",
    "    'w_max': 6,\n",
    "    'lr': 0.2,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0,\n",
    "    'spectral_wd': 0,\n",
    "    'input_dim': 32 * 32 * 3,\n",
    "    'output_dim': 10,\n",
    "    'd_embed': 256,\n",
    "    'num_blocks': 3,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'sensitive_to_wmax': {'default': True},  # False for spec_hammer\n",
    "    'data': 'cifar',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 100,\n",
    "    'batch_size': 512,\n",
    "    'steps': 2000,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': True,\n",
    "    'log_interval': 50,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "cifar_mlp_adam_unconstrained = {\n",
    "    'optimizer': 'adam',\n",
    "    'project': {'default': 'none'},\n",
    "    'w_max': 1,\n",
    "    'lr': 0.0013,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0.08,\n",
    "    'spectral_wd': 0,\n",
    "    'input_dim': 32 * 32 * 3,\n",
    "    'output_dim': 10,\n",
    "    'd_embed': 256,\n",
    "    'num_blocks': 3,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'sensitive_to_wmax': {'default': False},\n",
    "    'data': 'cifar',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 100, \n",
    "    'batch_size': 512,\n",
    "    'steps': 2000,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': False,\n",
    "    'log_interval': 50,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "shakespeare_gpt_muon_constrained = {\n",
    "    'optimizer': 'muon',  # or adam\n",
    "    'project': {'default': 'soft_cap'},  # specify per-layer (none, soft_cap, spec_normalize, etc.)\n",
    "    'w_max': 6,\n",
    "    'lr': 0.1,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0,\n",
    "    'spectral_wd': 0,\n",
    "    'd_embed': 256,\n",
    "    'seq_len': 256,\n",
    "    'num_blocks': 3,\n",
    "    'num_heads': 4,\n",
    "    'softmax_scale': 1,\n",
    "    'final_scale': 1,\n",
    "    'residual_scale': 1,\n",
    "    'scales_learnable': False,\n",
    "    'blocks_mass': 16,\n",
    "    'layernorm_substitute': 'none',  # no layer norm\n",
    "    'max_embed_inflation_factor': 16,  # prevents embedding gradient columns from increasing too much under dualization\n",
    "    'use_unembed': False,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'sensitive_to_wmax': {'default': True},  # False for spec_hammer\n",
    "    'data': 'shakespeare',\n",
    "    'vocab_size': 65,\n",
    "    'model': 'gpt',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 100,\n",
    "    'batch_size': 512,\n",
    "    'steps': 2000,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': True,\n",
    "    'log_interval': 1,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "cifar_mlp_constrained_config = parse_config_from_json(cifar_mlp_muon_constrained)\n",
    "cifar_mlp_unconstrained_config = parse_config_from_json(cifar_mlp_adam_unconstrained)\n",
    "shakespeare_gpt_constrained_config = parse_config_from_json(shakespeare_gpt_muon_constrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fde6ea",
   "metadata": {},
   "source": [
    "Specify here which config you want to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc20c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = shakespeare_gpt_constrained_config\n",
    "# config = cifar_mlp_constrained_config\n",
    "# config = cifar_mlp_unconstrained_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1194381",
   "metadata": {},
   "source": [
    "Set up experiment and initialize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e305be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:2025-07-13 06:55:53,702:jax._src.xla_bridge:444: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/laker/lipschitz-transformers/lipschitz/lib/python3.13/site-packages/jax/_src/xla_bridge.py\", line 442, in discover_pjrt_plugins\n",
      "    plugin_module.initialize()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/laker/lipschitz-transformers/lipschitz/lib/python3.13/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 324, in initialize\n",
      "    _check_cuda_versions(raise_on_first_error=True)\n",
      "    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laker/lipschitz-transformers/lipschitz/lib/python3.13/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 281, in _check_cuda_versions\n",
      "    local_device_count = cuda_versions.cuda_device_count()\n",
      "RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: Unknown CUDA error 303; cuGetErrorName failed. This probably means that JAX was unable to load the CUDA libraries.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "477a30ee-9242-4207-a1f7-c8e4b5c702b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, loss_fn = get_data_loader(config)\n",
    "model = create_model(config)\n",
    "model.jit()\n",
    "optimizer = get_optimizer(config)\n",
    "logger = Logger(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11403d0c",
   "metadata": {},
   "source": [
    "Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e2a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "params = model.initialize(subkey)\n",
    "opt_state = optimizer.init_state(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7935d76",
   "metadata": {},
   "source": [
    "Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea1088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 20 atoms and 68 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 18.0 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    loss_fn = loss_fn,\n",
    "    config = config,\n",
    "    logger = logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57013ff",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e90277",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m params, opt_state, key = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m results = logger.get_results()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lipschitz-transformers/trainer.py:83\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(self, params, opt_state, key)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lipschitz-transformers/utils.py:101\u001b[39m, in \u001b[36mLogger.log_training\u001b[39m\u001b[34m(self, step, loss, accuracy)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.results[\u001b[33m\"\u001b[39m\u001b[33mlosses\u001b[39m\u001b[33m\"\u001b[39m].append(\u001b[38;5;28mfloat\u001b[39m(loss))\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28mself\u001b[39m.results[\u001b[33m\"\u001b[39m\u001b[33mtrain_accuracies\u001b[39m\u001b[33m\"\u001b[39m].append(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Print log message\u001b[39;00m\n\u001b[32m    104\u001b[39m memory_stats = jax.device_get(jax.devices()[\u001b[32m0\u001b[39m].memory_stats())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lipschitz-transformers/lipschitz/lib/python3.13/site-packages/jax/_src/array.py:310\u001b[39m, in \u001b[36mArrayImpl.__float__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__float__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    309\u001b[39m   core.check_scalar_conversion(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m.\u001b[34m__float__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lipschitz-transformers/lipschitz/lib/python3.13/site-packages/jax/_src/profiler.py:354\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    353\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lipschitz-transformers/lipschitz/lib/python3.13/site-packages/jax/_src/array.py:644\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.is_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    643\u001b[39m       \u001b[38;5;28mself\u001b[39m.sharding._internal_device_list.addressable_device_list):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     npy_value, did_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m     npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "params, opt_state, key = trainer.train(params, opt_state, key)\n",
    "\n",
    "results = logger.get_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipschitz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
