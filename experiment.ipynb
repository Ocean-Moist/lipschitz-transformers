{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd286c88-ce33-4be7-8aec-3c3fe5176c40",
   "metadata": {},
   "source": [
    "# Training a Lipschitz constrained model\n",
    "\n",
    "This notebook has two settings to choose from:\n",
    "1. MLP on CIFAR-10\n",
    "2. Transformer on Shakespeare text\n",
    "\n",
    "Within these configs, you can set the optimizer (AdamW, Muon), the weight norm constraint method (none, spectral capping, spectral normalization), and other hyperparameters.\n",
    "\n",
    "To train a 145M parameter transformer, check out the `/nanogpt` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "842cc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "from configs import parse_config_from_json\n",
    "from data_loaders import get_data_loader\n",
    "from models import create_model\n",
    "from optimizers import get_optimizer\n",
    "from trainer import Trainer\n",
    "from utils import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87ea7a",
   "metadata": {},
   "source": [
    "Specify the training setup. All the options are available in `configs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5ea2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_mlp_muon_constrained = {\n",
    "    'optimizer': 'muon',  # or adam\n",
    "    'project': {'default': 'soft_cap'},  # specify per-layer (none, soft_cap, spec_normalize, etc.)\n",
    "    'w_max': 6,\n",
    "    'lr': 0.2,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0,\n",
    "    'spectral_wd': 0,\n",
    "    'input_dim': 32 * 32 * 3,\n",
    "    'output_dim': 10,\n",
    "    'd_embed': 256,\n",
    "    'num_blocks': 3,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'sensitive_to_wmax': {'default': True},  # False for spec_hammer\n",
    "    'data': 'cifar',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 100,\n",
    "    'batch_size': 512,\n",
    "    'steps': 2000,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': True,\n",
    "    'log_interval': 50,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "cifar_mlp_adam_unconstrained = {\n",
    "    'optimizer': 'adam',\n",
    "    'project': {'default': 'none'},\n",
    "    'w_max': 1,\n",
    "    'lr': 0.0013,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0.08,\n",
    "    'spectral_wd': 0,\n",
    "    'input_dim': 32 * 32 * 3,\n",
    "    'output_dim': 10,\n",
    "    'd_embed': 256,\n",
    "    'num_blocks': 3,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'sensitive_to_wmax': {'default': False},\n",
    "    'data': 'cifar',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 100, \n",
    "    'batch_size': 512,\n",
    "    'steps': 2000,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': False,\n",
    "    'log_interval': 50,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "shakespeare_gpt_muon_constrained = {\n",
    "    'optimizer': 'muon',  # or adam\n",
    "    'project': {'default': 'soft_cap'},  # specify per-layer (none, soft_cap, spec_normalize, etc.)\n",
    "    'w_max': 6,\n",
    "    'lr': 0.1,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0,\n",
    "    'spectral_wd': 0,\n",
    "    'd_embed': 256,\n",
    "    'seq_len': 256,\n",
    "    'num_blocks': 3,\n",
    "    'num_heads': 4,\n",
    "    'softmax_scale': 1,\n",
    "    'final_scale': 1,\n",
    "    'residual_scale': 1,\n",
    "    'scales_learnable': False,\n",
    "    'blocks_mass': 16,\n",
    "    'layernorm_substitute': 'none',  # no layer norm\n",
    "    'max_embed_inflation_factor': 16,  # prevents embedding gradient columns from increasing too much under dualization\n",
    "    'use_unembed': False,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'sensitive_to_wmax': {'default': True},  # False for spec_hammer\n",
    "    'data': 'shakespeare',\n",
    "    'vocab_size': 65,\n",
    "    'model': 'gpt',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 100,\n",
    "    'batch_size': 512,\n",
    "    'steps': 2000,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': True,\n",
    "    'log_interval': 1,\n",
    "    'schedule': 'linear'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fde6ea",
   "metadata": {},
   "source": [
    "Specify here which config you want to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc20c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = shakespeare_gpt_constrained\n",
    "config_dict = cifar_mlp_muon_constrained\n",
    "# config = cifar_mlp_adam_unconstrained\n",
    "\n",
    "config = parse_config_from_json(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1194381",
   "metadata": {},
   "source": [
    "Set up experiment and initialize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e305be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "477a30ee-9242-4207-a1f7-c8e4b5c702b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, loss_fn = get_data_loader(config)\n",
    "model = create_model(config)\n",
    "model.jit()\n",
    "optimizer = get_optimizer(config)\n",
    "logger = Logger(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11403d0c",
   "metadata": {},
   "source": [
    "Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5e2a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "params = model.initialize(subkey)\n",
    "opt_state = optimizer.init_state(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7935d76",
   "metadata": {},
   "source": [
    "Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7ea1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    loss_fn = loss_fn,\n",
    "    config = config,\n",
    "    logger = logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57013ff",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9e90277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:31:44 gpu -1.0G ram 2.7G] Step:50/2000 train_loss:1.6408 train_acc:0.4727 ETA:00:16:01\n",
      "[22:32:07 gpu -1.0G ram 2.7G] Step:100/2000 train_loss:1.4056 train_acc:0.5332 ETA:00:15:05\n",
      "  Step:100/2000 val_loss:1.4424 val_acc:0.4928\n",
      "[22:32:34 gpu -1.0G ram 2.7G] Step:150/2000 train_loss:1.3619 train_acc:0.5859 ETA:00:15:13\n",
      "[22:32:57 gpu -1.0G ram 2.7G] Step:200/2000 train_loss:1.1653 train_acc:0.6172 ETA:00:14:36\n",
      "  Step:200/2000 val_loss:1.3741 val_acc:0.5083\n",
      "[22:33:23 gpu -1.0G ram 2.7G] Step:250/2000 train_loss:1.2460 train_acc:0.6270 ETA:00:14:25\n",
      "[22:33:47 gpu -1.0G ram 2.7G] Step:300/2000 train_loss:1.0298 train_acc:0.6934 ETA:00:13:51\n",
      "  Step:300/2000 val_loss:1.3391 val_acc:0.5273\n",
      "[22:34:13 gpu -1.0G ram 2.7G] Step:350/2000 train_loss:1.1002 train_acc:0.6406 ETA:00:13:36\n",
      "[22:34:36 gpu -1.0G ram 2.7G] Step:400/2000 train_loss:0.9660 train_acc:0.7500 ETA:00:13:06\n",
      "  Step:400/2000 val_loss:1.2954 val_acc:0.5447\n",
      "[22:35:03 gpu -1.0G ram 2.7G] Step:450/2000 train_loss:1.0783 train_acc:0.6914 ETA:00:12:47\n",
      "[22:35:26 gpu -1.0G ram 2.7G] Step:500/2000 train_loss:0.8975 train_acc:0.7598 ETA:00:12:19\n",
      "  Step:500/2000 val_loss:1.2971 val_acc:0.5442\n",
      "[22:35:53 gpu -1.0G ram 2.7G] Step:550/2000 train_loss:1.0394 train_acc:0.6914 ETA:00:11:59\n",
      "[22:36:16 gpu -1.0G ram 2.7G] Step:600/2000 train_loss:0.8468 train_acc:0.8105 ETA:00:11:31\n",
      "  Step:600/2000 val_loss:1.2917 val_acc:0.5524\n",
      "[22:36:42 gpu -1.0G ram 2.7G] Step:650/2000 train_loss:0.9499 train_acc:0.7324 ETA:00:11:09\n",
      "[22:37:06 gpu -1.0G ram 2.8G] Step:700/2000 train_loss:0.8582 train_acc:0.7852 ETA:00:10:42\n",
      "  Step:700/2000 val_loss:1.2832 val_acc:0.5532\n",
      "[22:37:32 gpu -1.0G ram 2.8G] Step:750/2000 train_loss:0.8788 train_acc:0.7480 ETA:00:10:20\n",
      "[22:37:55 gpu -1.0G ram 2.8G] Step:800/2000 train_loss:0.8121 train_acc:0.7871 ETA:00:09:53\n",
      "  Step:800/2000 val_loss:1.2806 val_acc:0.5488\n",
      "[22:38:22 gpu -1.0G ram 2.8G] Step:850/2000 train_loss:0.8866 train_acc:0.7715 ETA:00:09:30\n",
      "[22:38:45 gpu -1.0G ram 2.8G] Step:900/2000 train_loss:0.7864 train_acc:0.8203 ETA:00:09:03\n",
      "  Step:900/2000 val_loss:1.2756 val_acc:0.5499\n",
      "[22:39:11 gpu -1.0G ram 2.8G] Step:950/2000 train_loss:0.7372 train_acc:0.8105 ETA:00:08:41\n",
      "[22:39:35 gpu -1.0G ram 2.8G] Step:1000/2000 train_loss:0.7671 train_acc:0.8047 ETA:00:08:14\n",
      "  Step:1000/2000 val_loss:1.2522 val_acc:0.5606\n",
      "[22:40:01 gpu -1.0G ram 2.8G] Step:1050/2000 train_loss:0.8304 train_acc:0.7812 ETA:00:07:51\n",
      "[22:40:24 gpu -1.0G ram 2.8G] Step:1100/2000 train_loss:0.7933 train_acc:0.8027 ETA:00:07:25\n",
      "  Step:1100/2000 val_loss:1.2552 val_acc:0.5595\n",
      "[22:40:50 gpu -1.0G ram 2.8G] Step:1150/2000 train_loss:0.8032 train_acc:0.7832 ETA:00:07:01\n",
      "[22:41:13 gpu -1.0G ram 2.8G] Step:1200/2000 train_loss:0.7698 train_acc:0.8262 ETA:00:06:35\n",
      "  Step:1200/2000 val_loss:1.2333 val_acc:0.5740\n",
      "[22:41:39 gpu -1.0G ram 2.8G] Step:1250/2000 train_loss:0.7705 train_acc:0.8086 ETA:00:06:11\n",
      "[22:42:03 gpu -1.0G ram 2.8G] Step:1300/2000 train_loss:0.7101 train_acc:0.8594 ETA:00:05:46\n",
      "  Step:1300/2000 val_loss:1.2294 val_acc:0.5724\n",
      "[22:42:29 gpu -1.0G ram 2.8G] Step:1350/2000 train_loss:0.7397 train_acc:0.8359 ETA:00:05:22\n",
      "[22:42:53 gpu -1.0G ram 2.8G] Step:1400/2000 train_loss:0.6657 train_acc:0.8809 ETA:00:04:56\n",
      "  Step:1400/2000 val_loss:1.2297 val_acc:0.5760\n",
      "[22:43:19 gpu -1.0G ram 2.8G] Step:1450/2000 train_loss:0.7516 train_acc:0.8164 ETA:00:04:32\n",
      "[22:43:42 gpu -1.0G ram 2.8G] Step:1500/2000 train_loss:0.7181 train_acc:0.8301 ETA:00:04:07\n",
      "  Step:1500/2000 val_loss:1.2246 val_acc:0.5740\n",
      "[22:44:09 gpu -1.0G ram 2.8G] Step:1550/2000 train_loss:0.7266 train_acc:0.8438 ETA:00:03:43\n",
      "[22:44:32 gpu -1.0G ram 2.8G] Step:1600/2000 train_loss:0.6549 train_acc:0.8652 ETA:00:03:18\n",
      "  Step:1600/2000 val_loss:1.2227 val_acc:0.5736\n",
      "[22:44:59 gpu -1.0G ram 2.8G] Step:1650/2000 train_loss:0.5800 train_acc:0.8828 ETA:00:02:53\n",
      "[22:45:22 gpu -1.0G ram 2.8G] Step:1700/2000 train_loss:0.6774 train_acc:0.8750 ETA:00:02:28\n",
      "  Step:1700/2000 val_loss:1.2170 val_acc:0.5779\n",
      "[22:45:48 gpu -1.0G ram 2.8G] Step:1750/2000 train_loss:0.5889 train_acc:0.9023 ETA:00:02:04\n",
      "[22:46:11 gpu -1.0G ram 2.8G] Step:1800/2000 train_loss:0.6168 train_acc:0.8848 ETA:00:01:39\n",
      "  Step:1800/2000 val_loss:1.2151 val_acc:0.5788\n",
      "[22:46:38 gpu -1.0G ram 2.8G] Step:1850/2000 train_loss:0.6076 train_acc:0.8926 ETA:00:01:14\n",
      "[22:47:01 gpu -1.0G ram 2.8G] Step:1900/2000 train_loss:0.6035 train_acc:0.8789 ETA:00:00:49\n",
      "  Step:1900/2000 val_loss:1.2108 val_acc:0.5797\n",
      "[22:47:27 gpu -1.0G ram 2.8G] Step:1950/2000 train_loss:0.5675 train_acc:0.8965 ETA:00:00:24\n",
      "[22:47:51 gpu -1.0G ram 2.8G] Step:2000/2000 train_loss:0.6256 train_acc:0.8965 ETA:00:00:00\n",
      "  Step:2000/2000 val_loss:1.2078 val_acc:0.5832\n"
     ]
    }
   ],
   "source": [
    "params, opt_state, key = trainer.train(params, opt_state, key)\n",
    "\n",
    "results = logger.get_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
