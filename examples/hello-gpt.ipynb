{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, GPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modula can represent any neural net architecture. Here, let's see how to make a transformer that trains on Shakespeare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 64\n",
    "batch_size = 12\n",
    "\n",
    "from data.shakespeare import load_shakespeare\n",
    "\n",
    "data = load_shakespeare(context, batch_size)\n",
    "\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"val_loader\"]\n",
    "encode = data[\"encode\"]\n",
    "decode = data[\"decode\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at an example to verify the data loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (12, 64)\n",
      "Target shape: (12, 64)\n",
      "First input sequence: [41 53 50 42  1 40 50 53 53 42] ...\n",
      "First target sequence: [53 50 42  1 40 50 53 53 42  1] ...\n",
      "Decoded input: cold blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a \n",
      "Decoded target: old blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a p\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    print(\"First input sequence:\", inputs[0][:10], \"...\")\n",
    "    print(\"First target sequence:\", targets[0][:10], \"...\")\n",
    "    print(\"Decoded input:\", decode(inputs[0]))\n",
    "    print(\"Decoded target:\", decode(targets[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer hyperparameters\n",
    "\n",
    "vocab_size = 65\n",
    "num_heads = 4\n",
    "d_embed = 128\n",
    "d_query = 32\n",
    "d_value = 32\n",
    "num_blocks = 4\n",
    "attention_scale = 1\n",
    "final_scale = 1\n",
    "\n",
    "# training hyperparameters\n",
    "\n",
    "lr = 0.1\n",
    "beta = 0.95\n",
    "steps = 2001\n",
    "log_interval = 10\n",
    "val_interval = 100\n",
    "val_iters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We're ready to define our transformer. We'll give some extra care to *attention* and *residual connections*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention (TODO: clarify this explanation)\n",
    "\n",
    "We'll define attention with $1/d$ scaling instead of the usual $1/\\sqrt{d}$ scaling to make the attention block Lipschitz. That is to say, with the usual scaling, attention can become arbitrarily sensitive to changes in its input as $d$ grows. With $1/d$ scaling, the sensitivity is bounded at 3! This means the entire transformer can be well-normed. There are more details to this story. You can see more in \"Case Study I: Attention\" of our paper [Scalable Optimization in the Modular Norm](https://arxiv.org/pdf/2405.14813)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modula.abstract import Identity\n",
    "from modula.atom import Linear\n",
    "from modula.bond import SplitIntoHeads, MergeHeads, Rope, AttentionQK, CausalMask, Softmax, ApplyAttentionScores, GeLU\n",
    "\n",
    "def Attention(num_heads, d_embed, d_query, d_value, attention_scale):\n",
    "    \"\"\"Multi-head attention\"\"\"\n",
    "\n",
    "    # For keys, queries, and values we add a heads dimension. For the out projection, we remove heads.\n",
    "    # Remember modules compose right-to-left, and the order is Linear(d_out, d_in)! And @ means compose.\n",
    "    Q = SplitIntoHeads(num_heads) @ Linear(num_heads * d_query, d_embed)\n",
    "    K = SplitIntoHeads(num_heads) @ Linear(num_heads * d_query, d_embed)\n",
    "    V = SplitIntoHeads(num_heads) @ Linear(num_heads * d_value, d_embed)\n",
    "    W = Linear(d_embed, num_heads * d_value) @ MergeHeads()\n",
    "\n",
    "    # Read right-to-left: rotate (Q, K) with RoPE, apply Q @ K.T, mask, softmax (with a scale we can choose).\n",
    "    AttentionScores = Softmax(attention_scale) @ CausalMask() @ AttentionQK() @ Rope(d_query) @ (Q, K)\n",
    "\n",
    "    # Read right-to-left: apply attention scores, multiply by 1/3 to fix the sensitivity to 1, project back to d_embed.\n",
    "    return W @ (1/3 * ApplyAttentionScores()) @ (V, AttentionScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the sensitivity is 1 at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 4 atoms and 10 bonds\n",
      "...smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 4 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "print(Attention(num_heads, d_embed, d_query, d_value, attention_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections\n",
    "\n",
    "We'll define residual connections using a convex combination: if $L$ is the number of residual blocks, then we use the convex combination of the identity and the block to get $x = \\frac{L-1}{L} x + \\frac{1}{L} \\textsf{block}(x).$ Again the purpose is to control the sensitivity of the transformer. As the number of blocks $L$ grows, the norm of $x$ remains bounded. And the contribution from any particular block decays by at most $1/e$ through the entire network, because $(1 - 1/L)^L > 1/e$ for any depth $L$.\n",
    "\n",
    "In short, these changes allows us to control the input sensitivity of our transformer as it scales. We're ready to implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modula.atom import Embed\n",
    "\n",
    "def GPT(vocab_size, num_heads, d_embed, d_query, d_value, num_blocks, blocks_mass=5, attention_scale=1.0, final_scale=1.0):\n",
    "    # Set embed to have mass 1. This controls the proportion of feature learning that it contributes to the whole network.\n",
    "    embed = Embed(d_embed, vocab_size)\n",
    "    embed.tare()\n",
    "\n",
    "    # Let's create attention and MLP layers. \n",
    "    att = Attention(num_heads, d_embed, d_query, d_value, attention_scale)\n",
    "    mlp = Linear(d_embed, 4*d_embed) @ GeLU() @ Linear(4*d_embed, d_embed)\n",
    "\n",
    "    # For our residual connections, L = 2*num_blocks because each block has two residual connections.\n",
    "    att_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * att\n",
    "    mlp_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * mlp\n",
    "\n",
    "    # We can use powers of a module to compose it with itself many times!\n",
    "    blocks = (mlp_block @ att_block) ** num_blocks\n",
    "\n",
    "    # Set all transformer blocks to have mass 5 (by default).\n",
    "    # So 5/7 of the change in the network output is due to the blocks,\n",
    "    # and 2/7 of the change in output is due to the embedding and out projection.\n",
    "    blocks.tare(absolute=blocks_mass)\n",
    "\n",
    "    out = final_scale * Linear(vocab_size, d_embed)\n",
    "\n",
    "    return out @ blocks @ embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct our GPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 26 atoms and 78 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 7.0 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    d_embed=d_embed,\n",
    "    d_query=d_query,\n",
    "    d_value=d_value,\n",
    "    num_blocks=num_blocks,\n",
    "    attention_scale=attention_scale,\n",
    "    final_scale=final_scale,\n",
    ")\n",
    "\n",
    "model.jit()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and training\n",
    "\n",
    "To train our transformer we'll use cross entropy loss, which we can compute by decomposing the softmax:\n",
    "\n",
    "$$\n",
    "-\\log(\\text{target probability}) = -\\log(\\text{softmax}(\\text{logits})_\\text{target}) = -\\text{logit}_\\text{target} + \\text{logsumexp}(\\text{logits})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def cross_entropy_loss(w, inputs, targets):\n",
    "    # We use the logsumexp trick for stable cross entropy\n",
    "    logits = model(inputs, w)  # shape is [batch, seq_len, vocab_size]\n",
    "    batch_indices = jnp.arange(logits.shape[0])[:, None]  # shape is [batch, 1]\n",
    "    seq_indices = jnp.arange(logits.shape[1])[None, :]    # shape is [1, seq_len]\n",
    "    # This indexing selects out logits[b, s, targets[b, s]], which is the target logit\n",
    "    losses = -logits[batch_indices, seq_indices, targets] + jax.nn.logsumexp(logits, axis=-1)  # shape is [batch, seq_len]\n",
    "    return losses.mean()\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(cross_entropy_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 4.226325988769531\n",
      "--> val loss 4.179544925689697\n",
      "Step 10: loss 3.873875379562378\n",
      "Step 20: loss 3.3448638916015625\n",
      "Step 30: loss 2.8050029277801514\n",
      "Step 40: loss 2.68573260307312\n",
      "Step 50: loss 2.6098484992980957\n",
      "Step 60: loss 2.40746808052063\n",
      "Step 70: loss 2.4183788299560547\n",
      "Step 80: loss 2.359755039215088\n",
      "Step 90: loss 2.268550157546997\n",
      "Step 100: loss 2.3140738010406494\n",
      "--> val loss 2.542112112045288\n",
      "Step 110: loss 2.2838447093963623\n",
      "Step 120: loss 2.2057571411132812\n",
      "Step 130: loss 2.16951322555542\n",
      "Step 140: loss 2.274829864501953\n",
      "Step 150: loss 2.125481367111206\n",
      "Step 160: loss 2.2481937408447266\n",
      "Step 170: loss 2.173231601715088\n",
      "Step 180: loss 2.101841449737549\n",
      "Step 190: loss 2.1321706771850586\n",
      "Step 200: loss 2.127584457397461\n",
      "--> val loss 2.3946735858917236\n",
      "Step 210: loss 2.076207160949707\n",
      "Step 220: loss 2.1188466548919678\n",
      "Step 230: loss 1.9753059148788452\n",
      "Step 240: loss 2.097914457321167\n",
      "Step 250: loss 1.9066812992095947\n",
      "Step 260: loss 2.082390308380127\n",
      "Step 270: loss 2.067826271057129\n",
      "Step 280: loss 2.0280327796936035\n",
      "Step 290: loss 2.0601515769958496\n",
      "Step 300: loss 1.9718117713928223\n",
      "--> val loss 2.2931230068206787\n",
      "Step 310: loss 2.033405065536499\n",
      "Step 320: loss 2.087334632873535\n",
      "Step 330: loss 2.155350685119629\n",
      "Step 340: loss 2.1034016609191895\n",
      "Step 350: loss 1.9781570434570312\n",
      "Step 360: loss 1.9587877988815308\n",
      "Step 370: loss 2.0594890117645264\n",
      "Step 380: loss 2.022892475128174\n",
      "Step 390: loss 2.01819109916687\n",
      "Step 400: loss 2.0659756660461426\n",
      "--> val loss 2.3004090785980225\n",
      "Step 410: loss 2.056656837463379\n",
      "Step 420: loss 1.798985481262207\n",
      "Step 430: loss 1.9753262996673584\n",
      "Step 440: loss 1.945596694946289\n",
      "Step 450: loss 2.0218348503112793\n",
      "Step 460: loss 1.9617671966552734\n",
      "Step 470: loss 2.0201029777526855\n",
      "Step 480: loss 1.9355710744857788\n",
      "Step 490: loss 1.9824126958847046\n",
      "Step 500: loss 2.1090340614318848\n",
      "--> val loss 2.2479541301727295\n",
      "Step 510: loss 2.110130548477173\n",
      "Step 520: loss 1.9807603359222412\n",
      "Step 530: loss 2.021629810333252\n",
      "Step 540: loss 1.9269845485687256\n",
      "Step 550: loss 2.101534366607666\n",
      "Step 560: loss 1.9359185695648193\n",
      "Step 570: loss 1.930455207824707\n",
      "Step 580: loss 1.9491934776306152\n",
      "Step 590: loss 1.8894838094711304\n",
      "Step 600: loss 2.0024423599243164\n",
      "--> val loss 2.2411861419677734\n",
      "Step 610: loss 2.0775680541992188\n",
      "Step 620: loss 1.9754149913787842\n",
      "Step 630: loss 1.9710254669189453\n",
      "Step 640: loss 1.9638770818710327\n",
      "Step 650: loss 2.0011515617370605\n",
      "Step 660: loss 2.037271022796631\n",
      "Step 670: loss 1.9816073179244995\n",
      "Step 680: loss 1.94729745388031\n",
      "Step 690: loss 1.9141349792480469\n",
      "Step 700: loss 1.8173141479492188\n",
      "--> val loss 2.1446406841278076\n",
      "Step 710: loss 2.0077197551727295\n",
      "Step 720: loss 2.0506162643432617\n",
      "Step 730: loss 1.866899847984314\n",
      "Step 740: loss 2.007718563079834\n",
      "Step 750: loss 1.9971487522125244\n",
      "Step 760: loss 1.983358383178711\n",
      "Step 770: loss 1.8463720083236694\n",
      "Step 780: loss 2.0902762413024902\n",
      "Step 790: loss 1.918989896774292\n",
      "Step 800: loss 1.9571423530578613\n",
      "--> val loss 2.1562488079071045\n",
      "Step 810: loss 1.9854816198349\n",
      "Step 820: loss 1.9495376348495483\n",
      "Step 830: loss 1.971592664718628\n",
      "Step 840: loss 1.9227741956710815\n",
      "Step 850: loss 1.906704306602478\n",
      "Step 860: loss 1.8426916599273682\n",
      "Step 870: loss 1.9126386642456055\n",
      "Step 880: loss 2.0233521461486816\n",
      "Step 890: loss 2.089782476425171\n",
      "Step 900: loss 2.005467414855957\n",
      "--> val loss 2.185619592666626\n",
      "Step 910: loss 2.039752960205078\n",
      "Step 920: loss 1.9936355352401733\n",
      "Step 930: loss 1.9742443561553955\n",
      "Step 940: loss 1.8490054607391357\n",
      "Step 950: loss 1.9517892599105835\n",
      "Step 960: loss 2.0008292198181152\n",
      "Step 970: loss 1.9142072200775146\n",
      "Step 980: loss 1.8914082050323486\n",
      "Step 990: loss 1.9594504833221436\n",
      "Step 1000: loss 1.8839712142944336\n",
      "--> val loss 2.1235454082489014\n",
      "Step 1010: loss 1.9544671773910522\n",
      "Step 1020: loss 1.9478448629379272\n",
      "Step 1030: loss 1.877770185470581\n",
      "Step 1040: loss 1.9825094938278198\n",
      "Step 1050: loss 1.9515870809555054\n",
      "Step 1060: loss 1.8871911764144897\n",
      "Step 1070: loss 1.9803861379623413\n",
      "Step 1080: loss 1.962784767150879\n",
      "Step 1090: loss 1.9482669830322266\n",
      "Step 1100: loss 1.8175694942474365\n",
      "--> val loss 2.188453197479248\n",
      "Step 1110: loss 1.973239779472351\n",
      "Step 1120: loss 1.9945707321166992\n",
      "Step 1130: loss 1.831014633178711\n",
      "Step 1140: loss 1.904179334640503\n",
      "Step 1150: loss 2.075270175933838\n",
      "Step 1160: loss 1.7382800579071045\n",
      "Step 1170: loss 1.9723962545394897\n",
      "Step 1180: loss 1.909261703491211\n",
      "Step 1190: loss 1.9164950847625732\n",
      "Step 1200: loss 1.9078404903411865\n",
      "--> val loss 2.1128580570220947\n",
      "Step 1210: loss 1.8880428075790405\n",
      "Step 1220: loss 1.9012784957885742\n",
      "Step 1230: loss 1.9811797142028809\n",
      "Step 1240: loss 1.8212556838989258\n",
      "Step 1250: loss 1.94198739528656\n",
      "Step 1260: loss 1.7346700429916382\n",
      "Step 1270: loss 1.9966849088668823\n",
      "Step 1280: loss 1.9411163330078125\n",
      "Step 1290: loss 1.8707473278045654\n",
      "Step 1300: loss 1.8689401149749756\n",
      "--> val loss 2.0665194988250732\n",
      "Step 1310: loss 1.94491708278656\n",
      "Step 1320: loss 1.8864645957946777\n",
      "Step 1330: loss 1.898820161819458\n",
      "Step 1340: loss 1.9100055694580078\n",
      "Step 1350: loss 1.8633761405944824\n",
      "Step 1360: loss 1.767814040184021\n",
      "Step 1370: loss 1.8995506763458252\n",
      "Step 1380: loss 1.9218333959579468\n",
      "Step 1390: loss 1.8431390523910522\n",
      "Step 1400: loss 1.8286514282226562\n",
      "--> val loss 2.0196521282196045\n",
      "Step 1410: loss 1.8182090520858765\n",
      "Step 1420: loss 1.887113332748413\n",
      "Step 1430: loss 1.829363226890564\n",
      "Step 1440: loss 1.8334993124008179\n",
      "Step 1450: loss 1.810379981994629\n",
      "Step 1460: loss 1.775273323059082\n",
      "Step 1470: loss 1.755607008934021\n",
      "Step 1480: loss 1.8457273244857788\n",
      "Step 1490: loss 1.7544348239898682\n",
      "Step 1500: loss 1.7605022192001343\n",
      "--> val loss 2.053795576095581\n",
      "Step 1510: loss 1.8187369108200073\n",
      "Step 1520: loss 1.8355510234832764\n",
      "Step 1530: loss 1.786482810974121\n",
      "Step 1540: loss 1.834463119506836\n",
      "Step 1550: loss 1.68904709815979\n",
      "Step 1560: loss 1.801898717880249\n",
      "Step 1570: loss 1.9403748512268066\n",
      "Step 1580: loss 1.8217511177062988\n",
      "Step 1590: loss 1.692779302597046\n",
      "Step 1600: loss 1.8987010717391968\n",
      "--> val loss 2.0065090656280518\n",
      "Step 1610: loss 1.7845790386199951\n",
      "Step 1620: loss 1.6856526136398315\n",
      "Step 1630: loss 1.7359598875045776\n",
      "Step 1640: loss 1.7776907682418823\n",
      "Step 1650: loss 1.7833908796310425\n",
      "Step 1660: loss 1.7566150426864624\n",
      "Step 1670: loss 1.647709846496582\n",
      "Step 1680: loss 1.7952165603637695\n",
      "Step 1690: loss 1.7902768850326538\n",
      "Step 1700: loss 1.6343061923980713\n",
      "--> val loss 1.9598098993301392\n",
      "Step 1710: loss 1.72713041305542\n",
      "Step 1720: loss 1.870985984802246\n",
      "Step 1730: loss 1.731076955795288\n",
      "Step 1740: loss 1.623453140258789\n",
      "Step 1750: loss 1.694138526916504\n",
      "Step 1760: loss 1.793503761291504\n",
      "Step 1770: loss 1.729834794998169\n",
      "Step 1780: loss 1.7792075872421265\n",
      "Step 1790: loss 1.7157148122787476\n",
      "Step 1800: loss 1.6625142097473145\n",
      "--> val loss 1.9422310590744019\n",
      "Step 1810: loss 1.7238006591796875\n",
      "Step 1820: loss 1.6911277770996094\n",
      "Step 1830: loss 1.7266614437103271\n",
      "Step 1840: loss 1.7286752462387085\n",
      "Step 1850: loss 1.736509084701538\n",
      "Step 1860: loss 1.668350100517273\n",
      "Step 1870: loss 1.7246431112289429\n",
      "Step 1880: loss 1.6718215942382812\n",
      "Step 1890: loss 1.6807279586791992\n",
      "Step 1900: loss 1.5433459281921387\n",
      "--> val loss 1.924622893333435\n",
      "Step 1910: loss 1.6825320720672607\n",
      "Step 1920: loss 1.7276886701583862\n",
      "Step 1930: loss 1.7412971258163452\n",
      "Step 1940: loss 1.8325214385986328\n",
      "Step 1950: loss 1.765400767326355\n",
      "Step 1960: loss 1.7525584697723389\n",
      "Step 1970: loss 1.7861948013305664\n",
      "Step 1980: loss 1.6685152053833008\n",
      "Step 1990: loss 1.765364646911621\n",
      "Step 2000: loss 1.6752636432647705\n",
      "--> val loss 1.9063739776611328\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "w = model.initialize(key)\n",
    "\n",
    "step = 0\n",
    "momentum = [0 * weight for weight in w]\n",
    "lr_schedule = lambda step: lr * (steps - step) / steps\n",
    "for inputs, targets in train_loader:\n",
    "    loss, grad_w = loss_and_grad(w, inputs, targets)\n",
    "    momentum = [beta * m + (1 - beta) * g_w for m, g_w in zip(momentum, grad_w)]\n",
    "    d_w = model.dualize(momentum)\n",
    "    w = [weight - lr_schedule(step) * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: loss {loss}\")\n",
    "    \n",
    "    if step % val_interval == 0:\n",
    "        val_losses = []\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            loss, _ = loss_and_grad(w, val_inputs, val_targets)\n",
    "            val_losses.append(loss)\n",
    "            if len(val_losses) >= val_iters:\n",
    "                break\n",
    "        print(f\"--> val loss {sum(val_losses)/len(val_losses)}\")\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    if step >= steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Though this be madness, yet there is method in't\n",
    "\n",
    "And indeed, let us look at how our wee model stacks up the master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "\n",
      "If the heave elder\n",
      "That were the heave the fare the greath the wors,\n",
      "The provery the graint be will my\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 1:\n",
      "\n",
      "Ifens the content the and the say man\n",
      "The be stranger to heart, and I dain their comme the her all her\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 2:\n",
      "\n",
      "If the can heart the was constleman:\n",
      "In the proper the true the bloody.\n",
      "\n",
      "CLAUDITA:\n",
      "I parce then ountra\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_tokens=100, temperature=0.5, seed=0):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    tokens = jnp.array(encode(prompt))\n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(jnp.expand_dims(tokens, 0), w)\n",
    "        next_token_logits = logits[0, -1] / temperature\n",
    "        \n",
    "        # Sample from our model's token distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        next_token = jax.random.categorical(subkey, next_token_logits)\n",
    "        tokens = jnp.append(tokens, next_token)\n",
    "    \n",
    "    return decode(tokens)\n",
    "\n",
    "for seed in range(3):\n",
    "    print(f\"Sample {seed}:\\n\\n{generate_text('If', max_tokens=100, seed=seed)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, not too bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
