{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, GPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modula can represent any neural net architecture. Here, let's see how to make a transformer that trains on Shakespeare! Let's set out the model parameters first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karpathy's smallest GPT config\n",
    "\n",
    "vocab_size = 65\n",
    "context = 64\n",
    "num_heads = 4\n",
    "d_embed = 128\n",
    "d_query = 32\n",
    "d_value = 32\n",
    "num_blocks = 4\n",
    "attention_scale = 1\n",
    "final_scale = 1\n",
    "\n",
    "# training hyperparameters\n",
    "\n",
    "lr = 0.2\n",
    "beta = 0.0\n",
    "batch_size = 64\n",
    "steps = 401\n",
    "log_interval = 10\n",
    "val_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.shakespeare import load_shakespeare\n",
    "\n",
    "data = load_shakespeare(context, batch_size)\n",
    "\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"val_loader\"]\n",
    "encode = data[\"encode\"]\n",
    "decode = data[\"decode\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at an example to verify the data loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 64)\n",
      "Target shape: (64, 64)\n",
      "First input sequence: [41 53 50 42  1 40 50 53 53 42] ...\n",
      "First target sequence: [53 50 42  1 40 50 53 53 42  1] ...\n",
      "Decoded input: cold blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a \n",
      "Decoded target: old blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a p\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    print(\"First input sequence:\", inputs[0][:10], \"...\")\n",
    "    print(\"First target sequence:\", targets[0][:10], \"...\")\n",
    "    print(\"Decoded input:\", decode(inputs[0]))\n",
    "    print(\"Decoded target:\", decode(targets[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We're ready to define our transformer. We'll give some extra care to *attention* and *residual connections*.\n",
    "\n",
    "1. We'll define attention with $1/d$ scaling instead of the usual $1/\\sqrt{d}$ scaling to make the attention block Lipschitz. That is to say, with the usual scaling, attention can become arbitrarily sensitive to changes in its input as $d$ grows. With $1/\\sqrt{d}$ scaling, the sensitivity is bounded at 1 and the sharpness is bounded at 3! This means the entire transformer can be well-normed. The full story is in \"Case Study I: Attention\" of our paper ![Scalable Optimization in the Modular Norm](https://arxiv.org/pdf/2405.14813).\n",
    "\n",
    "2. We'll define residual connections using a linear combination: if $L$ is the number of residual blocks, then we use $x = \\frac{L-1}{L} x + \\frac{1}{L} \\textsf{block}(x).$ Again the purpose is to control the sensitivity of the transformer. As $L$ grows, the norm of $x$ remains bounded. And the contribution from any particular block decays by at most $1/e$ through the entire network, because $(1 - 1/L)^L > 1/e$ for any depth $L$.\n",
    "\n",
    "In short, these changes allow us to control the input sensitivity of our transformer as it scales. We're ready to implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 26 atoms and 78 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 7.0 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "from modula.abstract import Identity\n",
    "from modula.atom import Linear, Embed\n",
    "from modula.bond import SplitIntoHeads, MergeHeads, Rope, AttentionQK, CausalMask, Softmax, ApplyAttentionScores, GeLU\n",
    "\n",
    "def Attention(num_heads, d_embed, d_query, d_value, softmax_scale, causal):\n",
    "    \"\"\"Multi-head attention\"\"\"\n",
    "\n",
    "    # For keys, queries, and values we add a heads dimension. For the out projection, we remove heads.\n",
    "    # Remember modules compose right-to-left, and the order is Linear(d_out, d_in)!\n",
    "    Q = SplitIntoHeads(num_heads) @ Linear(num_heads * d_query, d_embed)\n",
    "    K = SplitIntoHeads(num_heads) @ Linear(num_heads * d_query, d_embed)\n",
    "    V = SplitIntoHeads(num_heads) @ Linear(num_heads * d_value, d_embed)\n",
    "    W = Linear(d_embed, num_heads * d_value) @ MergeHeads()\n",
    "\n",
    "    # Read right-to-left: rotate (Q, K) with RoPE, apply Q @ K.T, mask, softmax (with a scale we can choose).\n",
    "    AttentionScores = Softmax(softmax_scale) @ CausalMask() @ AttentionQK() @ Rope(d_query) @ (Q, K)\n",
    "\n",
    "    # Read right-to-left: apply attention scores, multiply by 1/3 to fix the sharpness to 1, project back to d_embed.\n",
    "    return W @ (1/3 * ApplyAttentionScores()) @ (V, AttentionScores)\n",
    "\n",
    "def GPT(vocab_size, num_heads, d_embed, d_query, d_value, num_blocks, blocks_mass=5, attention_scale=1.0, final_scale=1.0):\n",
    "    # Set embed to have mass 1. This controls the proportion of feature learning that occurs here.\n",
    "    embed = Embed(d_embed, vocab_size)\n",
    "    embed.tare()\n",
    "\n",
    "    # Let's create attention and MLP layers. \n",
    "    att = Attention(num_heads, d_embed, d_query, d_value, attention_scale, causal=True)\n",
    "    mlp = Linear(d_embed, 4*d_embed) @ GeLU() @ Linear(4*d_embed, d_embed)\n",
    "\n",
    "    # For our residual connections, L = 2*num_blocks because each block has two residual connections.\n",
    "    att_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * att\n",
    "    mlp_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * mlp\n",
    "\n",
    "    # We can use powers of a module to compose it with itself many times!\n",
    "    blocks = (mlp_block @ att_block) ** num_blocks\n",
    "\n",
    "    # Set all transformer blocks to have mass 5 (by default).\n",
    "    # So 5/7 of all feature learning occurs in the blocks,\n",
    "    # and 2/7 occurs in the embedding and out projection.\n",
    "    blocks.tare(absolute=blocks_mass)\n",
    "\n",
    "    out = final_scale * Linear(vocab_size, d_embed)\n",
    "\n",
    "    return out @ blocks @ embed\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    d_embed=d_embed,\n",
    "    d_query=d_query,\n",
    "    d_value=d_value,\n",
    "    num_blocks=num_blocks,\n",
    "    attention_scale=attention_scale,\n",
    "    final_scale=final_scale,\n",
    ")\n",
    "\n",
    "model.jit()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the loss function. We'll use cross entropy loss, which we can compute using the logsumexp trick:\n",
    "\n",
    "$$\n",
    "-\\log(\\text{target probability}) = -\\log(\\text{softmax}(\\text{logits})_\\text{target}) = -\\text{logit}_\\text{target} + \\text{logsumexp}(\\text{logits})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def cross_entropy_loss(w, inputs, targets):\n",
    "    # We use the logsumexp trick for stable cross entropy\n",
    "    logits = model(inputs, w)  # shape is [batch, seq_len, vocab_size]\n",
    "    batch_indices = jnp.arange(logits.shape[0])[:, None]  # shape is [batch, 1]\n",
    "    seq_indices = jnp.arange(logits.shape[1])[None, :]    # shape is [1, seq_len]\n",
    "    # This indexing selects out logits[b, s, targets[b, s]], which is the target logit\n",
    "    losses = -logits[batch_indices, seq_indices, targets] + jax.nn.logsumexp(logits, axis=-1)  # shape is [batch, seq_len]\n",
    "    return losses.mean()\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(cross_entropy_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 4.210641860961914\n",
      "--> val loss 4.149214267730713\n",
      "Step 10: loss 3.3135969638824463\n",
      "Step 20: loss 2.638061285018921\n",
      "Step 30: loss 2.4735331535339355\n",
      "Step 40: loss 2.352996826171875\n",
      "Step 50: loss 2.2717297077178955\n",
      "Step 60: loss 2.253775119781494\n",
      "Step 70: loss 2.179286241531372\n",
      "Step 80: loss 2.1060492992401123\n",
      "Step 90: loss 2.0014119148254395\n",
      "Step 100: loss 1.9917633533477783\n",
      "--> val loss 2.2128195762634277\n",
      "Step 110: loss 2.0247018337249756\n",
      "Step 120: loss 1.94600510597229\n",
      "Step 130: loss 1.9080532789230347\n",
      "Step 140: loss 1.9041284322738647\n",
      "Step 150: loss 1.8841463327407837\n",
      "Step 160: loss 1.8827264308929443\n",
      "Step 170: loss 1.8754804134368896\n",
      "Step 180: loss 1.817456603050232\n",
      "Step 190: loss 1.818677544593811\n",
      "Step 200: loss 1.7669349908828735\n",
      "--> val loss 1.9174721240997314\n",
      "Step 210: loss 1.768047571182251\n",
      "Step 220: loss 1.789674997329712\n",
      "Step 230: loss 1.7021842002868652\n",
      "Step 240: loss 1.743955373764038\n",
      "Step 250: loss 1.7292665243148804\n",
      "Step 260: loss 1.7314412593841553\n",
      "Step 270: loss 1.7567459344863892\n",
      "Step 280: loss 1.6791701316833496\n",
      "Step 290: loss 1.6610658168792725\n",
      "Step 300: loss 1.61663019657135\n",
      "--> val loss 1.8286421298980713\n",
      "Step 310: loss 1.6708993911743164\n",
      "Step 320: loss 1.6795330047607422\n",
      "Step 330: loss 1.6349070072174072\n",
      "Step 340: loss 1.6703948974609375\n",
      "Step 350: loss 1.65470552444458\n",
      "Step 360: loss 1.6376702785491943\n",
      "Step 370: loss 1.630429744720459\n",
      "Step 380: loss 1.6162567138671875\n",
      "Step 390: loss 1.6813067197799683\n",
      "Step 400: loss 1.665879487991333\n",
      "--> val loss 1.7853972911834717\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "w = model.initialize(key)\n",
    "\n",
    "step = 0\n",
    "momentum = [0 * weight for weight in w]\n",
    "lr_schedule = lambda step: lr * (steps - step) / steps\n",
    "for inputs, targets in train_loader:\n",
    "    loss, grad_w = loss_and_grad(w, inputs, targets)\n",
    "    momentum = [beta * m + (1 - beta) * g_w for m, g_w in zip(momentum, grad_w)]\n",
    "    d_w = model.dualize(momentum)\n",
    "    w = [weight - lr_schedule(step) * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: loss {loss}\")\n",
    "    \n",
    "    if step % val_interval == 0:\n",
    "        val_losses = []\n",
    "        val_batches = 10\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            loss, _ = loss_and_grad(w, val_inputs, val_targets)\n",
    "            val_losses.append(loss)\n",
    "            if len(val_losses) >= val_batches:\n",
    "                break\n",
    "        print(f\"--> val loss {sum(val_losses)/len(val_losses)}\")\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    if step >= steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a last check, let's see how our model stacks up to Shakespeare when it writes. Hey, not too bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:  IfOHAUS:\n",
      "First Merish \n",
      "--------------------------------------------------------------------------------\n",
      "Generated:  Ifamble, tho last,\n",
      "Bes\n",
      "--------------------------------------------------------------------------------\n",
      "Generated:  If\n",
      "Lor man beend the w\n",
      "--------------------------------------------------------------------------------\n",
      "Generated:  If you. Mercies.\n",
      "Lood \n",
      "--------------------------------------------------------------------------------\n",
      "Generated:  If let, didly ant womo\n",
      "--------------------------------------------------------------------------------\n",
      "Generated:  If wich with shy Say c\n",
      "--------------------------------------------------------------------------------\n",
      "Generated:  Ifry, Aumit? a to as n\n",
      "--------------------------------------------------------------------------------\n",
      "Generated:  If thy hows; brot have\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_tokens=100, temperature=1.0, seed=0):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    tokens = jnp.array(encode(prompt))\n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(jnp.expand_dims(tokens, 0), w)\n",
    "        next_token_logits = logits[0, -1] / temperature\n",
    "        \n",
    "        # Sample from our model's token distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        next_token = jax.random.categorical(subkey, next_token_logits)\n",
    "        tokens = jnp.append(tokens, next_token)\n",
    "    \n",
    "    return decode(tokens)\n",
    "\n",
    "for seed in range(8):\n",
    "    print(\"Generated: \", generate_text(\"If\", max_tokens=20, seed=seed))\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
