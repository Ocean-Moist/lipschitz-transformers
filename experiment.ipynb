{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd286c88-ce33-4be7-8aec-3c3fe5176c40",
   "metadata": {},
   "source": [
    "# Training a Lipschitz constrained model\n",
    "\n",
    "This notebook has two settings to choose from:\n",
    "1. MLP on CIFAR-10\n",
    "2. Transformer on Shakespeare text\n",
    "\n",
    "Within these configs, you can set the optimizer (AdamW, Muon), the weight norm constraint method (none, spectral capping, spectral normalization), and other hyperparameters.\n",
    "\n",
    "To train a 145M parameter transformer, check out the `/nanogpt` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "from configs import parse_config_from_json\n",
    "from data_loaders import get_data_loader\n",
    "from models import create_model\n",
    "from optimizers import get_optimizer\n",
    "from trainer import Trainer\n",
    "from utils import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87ea7a",
   "metadata": {},
   "source": [
    "Specify the training setup. All the options are available in `configs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_mlp_muon_constrained = {\n",
    "    'optimizer': 'muon',  # or adam\n",
    "    'project': {'default': 'soft_cap'},  # specify per-layer (none, soft_cap, spec_normalize, etc.)\n",
    "    'w_max': 6,\n",
    "    'lr': 0.2,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0,\n",
    "    'spectral_wd': 0,\n",
    "    'input_dim': 32 * 32 * 3,\n",
    "    'output_dim': 10,\n",
    "    'd_embed': 256,\n",
    "    'num_blocks': 3,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'sensitive_to_wmax': {'default': True},  # False for spec_hammer\n",
    "    'data': 'cifar',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 100,\n",
    "    'batch_size': 512,\n",
    "    'steps': 2000,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': True,\n",
    "    'log_interval': 50,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "cifar_mlp_adam_unconstrained = {\n",
    "    'optimizer': 'adam',\n",
    "    'project': {'default': 'none'},\n",
    "    'w_max': 1,\n",
    "    'lr': 0.0013,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0.08,\n",
    "    'spectral_wd': 0,\n",
    "    'input_dim': 32 * 32 * 3,\n",
    "    'output_dim': 10,\n",
    "    'd_embed': 256,\n",
    "    'num_blocks': 3,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'sensitive_to_wmax': {'default': False},\n",
    "    'data': 'cifar',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 100, \n",
    "    'batch_size': 512,\n",
    "    'steps': 2000,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': False,\n",
    "    'log_interval': 50,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "shakespeare_gpt_muon_constrained = {\n",
    "    'optimizer': 'muon',  # or adam\n",
    "    'project': {'default': 'soft_cap'},  # specify per-layer (none, soft_cap, spec_normalize, etc.)\n",
    "    'w_max': 6,\n",
    "    'lr': 0.1,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0,\n",
    "    'spectral_wd': 0,\n",
    "    'd_embed': 256,\n",
    "    'seq_len': 256,\n",
    "    'num_blocks': 3,\n",
    "    'num_heads': 4,\n",
    "    'softmax_scale': 1,\n",
    "    'final_scale': 1,\n",
    "    'residual_scale': 1,\n",
    "    'scales_learnable': False,\n",
    "    'blocks_mass': 16,\n",
    "    'layernorm_substitute': 'none',  # no layer norm\n",
    "    'max_embed_inflation_factor': 16,  # prevents embedding gradient columns from increasing too much under dualization\n",
    "    'use_unembed': False,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'sensitive_to_wmax': {'default': True},  # False for spec_hammer\n",
    "    'data': 'shakespeare',\n",
    "    'vocab_size': 65,\n",
    "    'model': 'gpt',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 100,\n",
    "    'batch_size': 512,\n",
    "    'steps': 2000,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': True,\n",
    "    'log_interval': 1,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "cifar_mlp_constrained_config = parse_config_from_json(cifar_mlp_muon_constrained)\n",
    "cifar_mlp_unconstrained_config = parse_config_from_json(cifar_mlp_adam_unconstrained)\n",
    "shakespeare_gpt_constrained_config = parse_config_from_json(shakespeare_gpt_muon_constrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fde6ea",
   "metadata": {},
   "source": [
    "Specify here which config you want to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc20c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = shakespeare_gpt_constrained_config\n",
    "# config = cifar_mlp_constrained_config\n",
    "# config = cifar_mlp_unconstrained_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1194381",
   "metadata": {},
   "source": [
    "Set up experiment and initialize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e305be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "477a30ee-9242-4207-a1f7-c8e4b5c702b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, loss_fn = get_data_loader(config)\n",
    "model = create_model(config)\n",
    "model.jit()\n",
    "optimizer = get_optimizer(config)\n",
    "logger = Logger(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11403d0c",
   "metadata": {},
   "source": [
    "Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5e2a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "params = model.initialize(subkey)\n",
    "opt_state = optimizer.init_state(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7935d76",
   "metadata": {},
   "source": [
    "Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7ea1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    loss_fn = loss_fn,\n",
    "    config = config,\n",
    "    logger = logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57013ff",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e90277",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, opt_state, key = trainer.train(params, opt_state, key)\n",
    "\n",
    "results = logger.get_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipschitz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
