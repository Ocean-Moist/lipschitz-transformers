{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a transformer with Modula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up the model parameters and the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karpathy's smallest GPT config\n",
    "\n",
    "vocab_size = 65\n",
    "context = 64\n",
    "num_heads = 4\n",
    "d_embed = 128\n",
    "d_query = 32\n",
    "d_value = 32\n",
    "num_blocks = 4\n",
    "softmax_scale = 1\n",
    "\n",
    "# training hyperparameters\n",
    "\n",
    "lr = 0.1\n",
    "beta = 0.95\n",
    "batch_size = 64\n",
    "steps = 201\n",
    "log_interval = 10\n",
    "val_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.shakespeare import load_shakespeare\n",
    "\n",
    "data = load_shakespeare(context, batch_size)\n",
    "\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"val_loader\"]\n",
    "encode = data[\"encode\"]\n",
    "decode = data[\"decode\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 64)\n",
      "Target shape: (64, 64)\n",
      "First input sequence: [41 53 50 42  1 40 50 53 53 42  1 52 53  1 57 54 39 56 49  1 53 44  1 46\n",
      " 53 52 53 59 56  1 40 47 42 43 57  8  0  0 26 27 30 32 20 33 25 14 17 30\n",
      " 24 13 26 16 10  0 14 43  1 58 46 53 59  1 39  1]\n",
      "First target sequence: [53 50 42  1 40 50 53 53 42  1 52 53  1 57 54 39 56 49  1 53 44  1 46 53\n",
      " 52 53 59 56  1 40 47 42 43 57  8  0  0 26 27 30 32 20 33 25 14 17 30 24\n",
      " 13 26 16 10  0 14 43  1 58 46 53 59  1 39  1 54]\n",
      "Decoded input: cold blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a \n",
      "Decoded target: old blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a p\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    print(\"First input sequence:\", inputs[0])\n",
    "    print(\"First target sequence:\", targets[0])\n",
    "    print(\"Decoded input:\", decode(inputs[0]))\n",
    "    print(\"Decoded target:\", decode(targets[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to define our transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 26 atoms and 77 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 7.0 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "from modula.compound import GPT\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    d_embed=d_embed,\n",
    "    d_query=d_query,\n",
    "    d_value=d_value,\n",
    "    num_blocks=num_blocks,\n",
    "    softmax_scale=softmax_scale\n",
    ")\n",
    "\n",
    "model.jit()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(w, inputs, targets):\n",
    "    logits = model(inputs, w)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
    "    return loss.mean()\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(cross_entropy_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 4.185632705688477\n",
      "Step 10: loss 3.7173829078674316\n",
      "Step 20: loss 3.070303440093994\n",
      "Step 30: loss 2.7101216316223145\n",
      "Step 40: loss 2.5390071868896484\n",
      "Step 50: loss 2.4373273849487305\n",
      "Step 60: loss 2.3451037406921387\n",
      "Step 70: loss 2.211345672607422\n",
      "Step 80: loss 2.15478515625\n",
      "Step 90: loss 2.0852365493774414\n",
      "Step 100: loss 2.057664155960083\n",
      "Step 110: loss 2.0198516845703125\n",
      "Step 120: loss 2.033675193786621\n",
      "Step 130: loss 1.918455719947815\n",
      "Step 140: loss 1.9736814498901367\n",
      "Step 150: loss 1.9408278465270996\n",
      "Step 160: loss 1.9075714349746704\n",
      "Step 170: loss 1.8821725845336914\n",
      "Step 180: loss 1.8457083702087402\n",
      "Step 190: loss 1.8166624307632446\n",
      "Step 200: loss 1.828275203704834\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "w = model.initialize(key)\n",
    "\n",
    "step = 0\n",
    "momentum = [0 * weight for weight in w]\n",
    "for inputs, targets in train_loader:\n",
    "    loss, grad_w = loss_and_grad(w, inputs, targets)\n",
    "    momentum = [beta * m + (1 - beta) * g_w for m, g_w in zip(momentum, grad_w)]\n",
    "    d_w = model.dualize(momentum)\n",
    "    w = [weight - lr * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: loss {loss}\")\n",
    "    \n",
    "    # if step % val_interval == 0:\n",
    "    #     val_losses = []\n",
    "    #     for val_inputs, val_targets in val_loader:\n",
    "    #         loss, _ = loss_and_grad(w, val_inputs, val_targets)\n",
    "    #         val_losses.append(loss)\n",
    "    #     print(f\"\\tval loss {sum(val_losses)/len(val_losses)}\")\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    if step >= steps:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
