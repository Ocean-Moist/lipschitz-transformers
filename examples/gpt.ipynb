{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, GPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modula can represent any neural net architecture. Here, let's see how to make a transformer that trains on Shakespeare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Shakespeare dataset...\n",
      "Processing Shakespeare dataset...\n",
      "Length of dataset in characters: 1,115,394\n",
      "Vocabulary size: 65\n",
      "Train has 1,003,854 tokens\n",
      "Val has 111,540 tokens\n",
      "Shakespeare dataset processing complete.\n"
     ]
    }
   ],
   "source": [
    "context = 64\n",
    "batch_size = 64\n",
    "\n",
    "from data.shakespeare import load_shakespeare\n",
    "\n",
    "data = load_shakespeare(context, batch_size)\n",
    "\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"val_loader\"]\n",
    "encode = data[\"encode\"]\n",
    "decode = data[\"decode\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at an example to verify the data loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 64)\n",
      "Target shape: (64, 64)\n",
      "First input sequence: [41 53 50 42  1 40 50 53 53 42] ...\n",
      "First target sequence: [53 50 42  1 40 50 53 53 42  1] ...\n",
      "Decoded input: cold blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a \n",
      "Decoded target: old blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a p\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    print(\"First input sequence:\", inputs[0][:10], \"...\")\n",
    "    print(\"First target sequence:\", targets[0][:10], \"...\")\n",
    "    print(\"Decoded input:\", decode(inputs[0]))\n",
    "    print(\"Decoded target:\", decode(targets[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer hyperparameters\n",
    "\n",
    "vocab_size = 65\n",
    "num_heads = 4\n",
    "d_embed = 128\n",
    "d_query = 32\n",
    "d_value = 32\n",
    "num_blocks = 4\n",
    "attention_scale = 1\n",
    "final_scale = 1\n",
    "\n",
    "# training hyperparameters\n",
    "\n",
    "lr = 0.1\n",
    "beta = 0.95\n",
    "steps = 2001\n",
    "log_interval = 10\n",
    "val_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We're ready to define our transformer. We'll give some extra care to *attention* and *residual connections*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention (TODO: clarify this explanation)\n",
    "\n",
    "We'll define attention with $1/d$ scaling instead of the usual $1/\\sqrt{d}$ scaling to make the attention block Lipschitz. That is to say, with the usual scaling, attention can become arbitrarily sensitive to changes in its input as $d$ grows. With $1/d$ scaling, the sensitivity is bounded at 3! This means the entire transformer can be well-normed. There are more details to this story. You can see more in \"Case Study I: Attention\" of our paper [Scalable Optimization in the Modular Norm](https://arxiv.org/pdf/2405.14813)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modula.abstract import Identity\n",
    "from modula.atom import Linear\n",
    "from modula.bond import SplitIntoHeads, MergeHeads, Rope, AttentionQK, CausalMask, Softmax, ApplyAttentionScores, GeLU\n",
    "\n",
    "def Attention(num_heads, d_embed, d_query, d_value, attention_scale):\n",
    "    \"\"\"Multi-head attention\"\"\"\n",
    "\n",
    "    # For keys, queries, and values we add a heads dimension. For the out projection, we remove heads.\n",
    "    # Remember modules compose right-to-left, and the order is Linear(d_out, d_in)! And @ means compose.\n",
    "    Q = SplitIntoHeads(num_heads) @ Linear(num_heads * d_query, d_embed)\n",
    "    K = SplitIntoHeads(num_heads) @ Linear(num_heads * d_query, d_embed)\n",
    "    V = SplitIntoHeads(num_heads) @ Linear(num_heads * d_value, d_embed)\n",
    "    W = Linear(d_embed, num_heads * d_value) @ MergeHeads()\n",
    "\n",
    "    # Read right-to-left: rotate (Q, K) with RoPE, apply Q @ K.T, mask, softmax (with a scale we can choose).\n",
    "    AttentionScores = Softmax(attention_scale) @ CausalMask() @ AttentionQK() @ Rope(d_query) @ (Q, K)\n",
    "\n",
    "    # Read right-to-left: apply attention scores, multiply by 1/3 to fix the sensitivity to 1, project back to d_embed.\n",
    "    return W @ (1/3 * ApplyAttentionScores()) @ (V, AttentionScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the sensitivity is 1 at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 4 atoms and 10 bonds\n",
      "...smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 4 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "print(Attention(num_heads, d_embed, d_query, d_value, attention_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections\n",
    "\n",
    "We'll define residual connections using a convex combination: if $L$ is the number of residual blocks, then we use the convex combination of the identity and the block to get $x = \\frac{L-1}{L} x + \\frac{1}{L} \\textsf{block}(x).$ Again the purpose is to control the sensitivity of the transformer. As the number of blocks $L$ grows, the norm of $x$ remains bounded. And the contribution from any particular block decays by at most $1/e$ through the entire network, because $(1 - 1/L)^L > 1/e$ for any depth $L$.\n",
    "\n",
    "In short, these changes allows us to control the input sensitivity of our transformer as it scales. We're ready to implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modula.atom import Embed\n",
    "\n",
    "def GPT(vocab_size, num_heads, d_embed, d_query, d_value, num_blocks, blocks_mass=5, attention_scale=1.0, final_scale=1.0):\n",
    "    # Set embed to have mass 1. This controls the proportion of feature learning that it contributes to the whole network.\n",
    "    embed = Embed(d_embed, vocab_size)\n",
    "    embed.tare()\n",
    "\n",
    "    # Let's create attention and MLP layers. \n",
    "    att = Attention(num_heads, d_embed, d_query, d_value, attention_scale)\n",
    "    mlp = Linear(d_embed, 4*d_embed) @ GeLU() @ Linear(4*d_embed, d_embed)\n",
    "\n",
    "    # For our residual connections, L = 2*num_blocks because each block has two residual connections.\n",
    "    att_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * att\n",
    "    mlp_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * mlp\n",
    "\n",
    "    # We can use powers of a module to compose it with itself many times!\n",
    "    blocks = (mlp_block @ att_block) ** num_blocks\n",
    "\n",
    "    # Set all transformer blocks to have mass 5 (by default).\n",
    "    # So 5/7 of the change in the network output is due to the blocks,\n",
    "    # and 2/7 of the change in output is due to the embedding and out projection.\n",
    "    blocks.tare(absolute=blocks_mass)\n",
    "\n",
    "    out = final_scale * Linear(vocab_size, d_embed)\n",
    "\n",
    "    return out @ blocks @ embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct our GPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 26 atoms and 78 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 7.0 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    d_embed=d_embed,\n",
    "    d_query=d_query,\n",
    "    d_value=d_value,\n",
    "    num_blocks=num_blocks,\n",
    "    attention_scale=attention_scale,\n",
    "    final_scale=final_scale,\n",
    ")\n",
    "\n",
    "model.jit()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and training\n",
    "\n",
    "To train our transformer we'll use cross entropy loss, which we can compute by decomposing the softmax:\n",
    "\n",
    "$$\n",
    "-\\log(\\text{target probability}) = -\\log(\\text{softmax}(\\text{logits})_\\text{target}) = -\\text{logit}_\\text{target} + \\text{logsumexp}(\\text{logits})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def cross_entropy_loss(w, inputs, targets):\n",
    "    # We use the logsumexp trick for stable cross entropy\n",
    "    logits = model(inputs, w)  # shape is [batch, seq_len, vocab_size]\n",
    "    batch_indices = jnp.arange(logits.shape[0])[:, None]  # shape is [batch, 1]\n",
    "    seq_indices = jnp.arange(logits.shape[1])[None, :]    # shape is [1, seq_len]\n",
    "    # This indexing selects out logits[b, s, targets[b, s]], which is the target logit\n",
    "    losses = -logits[batch_indices, seq_indices, targets] + jax.nn.logsumexp(logits, axis=-1)  # shape is [batch, seq_len]\n",
    "    return losses.mean()\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(cross_entropy_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 4.22137451171875\n",
      "--> val loss 4.1801252365112305\n",
      "Step 10: loss 3.8414347171783447\n",
      "Step 20: loss 3.2447433471679688\n",
      "Step 30: loss 2.783235788345337\n",
      "Step 40: loss 2.613834857940674\n",
      "Step 50: loss 2.48018741607666\n",
      "Step 60: loss 2.374842643737793\n",
      "Step 70: loss 2.288206100463867\n",
      "Step 80: loss 2.2245421409606934\n",
      "Step 90: loss 2.156920909881592\n",
      "Step 100: loss 2.1422369480133057\n",
      "--> val loss 2.2954983711242676\n",
      "Step 110: loss 2.1188039779663086\n",
      "Step 120: loss 2.0334277153015137\n",
      "Step 130: loss 1.9891002178192139\n",
      "Step 140: loss 2.0093936920166016\n",
      "Step 150: loss 1.9700260162353516\n",
      "Step 160: loss 1.9255917072296143\n",
      "Step 170: loss 1.9249231815338135\n",
      "Step 180: loss 1.9090293645858765\n",
      "Step 190: loss 1.9014718532562256\n",
      "Step 200: loss 1.8983714580535889\n",
      "--> val loss 2.0765464305877686\n",
      "Step 210: loss 1.9142591953277588\n",
      "Step 220: loss 1.8378630876541138\n",
      "Step 230: loss 1.851656436920166\n",
      "Step 240: loss 1.9034721851348877\n",
      "Step 250: loss 1.7876521348953247\n",
      "Step 260: loss 1.791902780532837\n",
      "Step 270: loss 1.7859854698181152\n",
      "Step 280: loss 1.7901076078414917\n",
      "Step 290: loss 1.748795509338379\n",
      "Step 300: loss 1.7753734588623047\n",
      "--> val loss 1.9566748142242432\n",
      "Step 310: loss 1.7503787279129028\n",
      "Step 320: loss 1.7456506490707397\n",
      "Step 330: loss 1.8085436820983887\n",
      "Step 340: loss 1.7869808673858643\n",
      "Step 350: loss 1.7652862071990967\n",
      "Step 360: loss 1.7783377170562744\n",
      "Step 370: loss 1.719006896018982\n",
      "Step 380: loss 1.7405461072921753\n",
      "Step 390: loss 1.7592098712921143\n",
      "Step 400: loss 1.7783526182174683\n",
      "--> val loss 1.9238208532333374\n",
      "Step 410: loss 1.7616539001464844\n",
      "Step 420: loss 1.7328360080718994\n",
      "Step 430: loss 1.7045857906341553\n",
      "Step 440: loss 1.6823840141296387\n",
      "Step 450: loss 1.7922757863998413\n",
      "Step 460: loss 1.7143800258636475\n",
      "Step 470: loss 1.7735986709594727\n",
      "Step 480: loss 1.7280588150024414\n",
      "Step 490: loss 1.7222384214401245\n",
      "Step 500: loss 1.7184430360794067\n",
      "--> val loss 1.8802999258041382\n",
      "Step 510: loss 1.7482819557189941\n",
      "Step 520: loss 1.7374401092529297\n",
      "Step 530: loss 1.6844133138656616\n",
      "Step 540: loss 1.6425913572311401\n",
      "Step 550: loss 1.7061121463775635\n",
      "Step 560: loss 1.6743921041488647\n",
      "Step 570: loss 1.6799324750900269\n",
      "Step 580: loss 1.6804696321487427\n",
      "Step 590: loss 1.6850496530532837\n",
      "Step 600: loss 1.6741182804107666\n",
      "--> val loss 1.8468875885009766\n",
      "Step 610: loss 1.695251703262329\n",
      "Step 620: loss 1.7496336698532104\n",
      "Step 630: loss 1.6737570762634277\n",
      "Step 640: loss 1.6837888956069946\n",
      "Step 650: loss 1.6649107933044434\n",
      "Step 660: loss 1.70197331905365\n",
      "Step 670: loss 1.7149096727371216\n",
      "Step 680: loss 1.7263270616531372\n",
      "Step 690: loss 1.7221121788024902\n",
      "Step 700: loss 1.7040144205093384\n",
      "--> val loss 1.8538627624511719\n",
      "Step 710: loss 1.6496294736862183\n",
      "Step 720: loss 1.7359435558319092\n",
      "Step 730: loss 1.6945018768310547\n",
      "Step 740: loss 1.749362826347351\n",
      "Step 750: loss 1.703830599784851\n",
      "Step 760: loss 1.7086752653121948\n",
      "Step 770: loss 1.7263286113739014\n",
      "Step 780: loss 1.7227504253387451\n",
      "Step 790: loss 1.6364881992340088\n",
      "Step 800: loss 1.6754353046417236\n",
      "--> val loss 1.8352693319320679\n",
      "Step 810: loss 1.6676571369171143\n",
      "Step 820: loss 1.6487020254135132\n",
      "Step 830: loss 1.6716536283493042\n",
      "Step 840: loss 1.6755403280258179\n",
      "Step 850: loss 1.674224853515625\n",
      "Step 860: loss 1.682645320892334\n",
      "Step 870: loss 1.6762577295303345\n",
      "Step 880: loss 1.7089228630065918\n",
      "Step 890: loss 1.7132154703140259\n",
      "Step 900: loss 1.6324317455291748\n",
      "--> val loss 1.8619581460952759\n",
      "Step 910: loss 1.6844971179962158\n",
      "Step 920: loss 1.7003483772277832\n",
      "Step 930: loss 1.6374602317810059\n",
      "Step 940: loss 1.7036571502685547\n",
      "Step 950: loss 1.6739215850830078\n",
      "Step 960: loss 1.6618021726608276\n",
      "Step 970: loss 1.6934523582458496\n",
      "Step 980: loss 1.6280627250671387\n",
      "Step 990: loss 1.6481183767318726\n",
      "Step 1000: loss 1.6435343027114868\n",
      "--> val loss 1.8022003173828125\n",
      "Step 1010: loss 1.683823585510254\n",
      "Step 1020: loss 1.645155906677246\n",
      "Step 1030: loss 1.6802992820739746\n",
      "Step 1040: loss 1.6708587408065796\n",
      "Step 1050: loss 1.626021146774292\n",
      "Step 1060: loss 1.651061773300171\n",
      "Step 1070: loss 1.6580737829208374\n",
      "Step 1080: loss 1.6319745779037476\n",
      "Step 1090: loss 1.7169984579086304\n",
      "Step 1100: loss 1.6611131429672241\n",
      "--> val loss 1.7608646154403687\n",
      "Step 1110: loss 1.6618778705596924\n",
      "Step 1120: loss 1.6172401905059814\n",
      "Step 1130: loss 1.6212753057479858\n",
      "Step 1140: loss 1.6672073602676392\n",
      "Step 1150: loss 1.6429001092910767\n",
      "Step 1160: loss 1.628511905670166\n",
      "Step 1170: loss 1.6039626598358154\n",
      "Step 1180: loss 1.688765048980713\n",
      "Step 1190: loss 1.6588584184646606\n",
      "Step 1200: loss 1.5917881727218628\n",
      "--> val loss 1.7562354803085327\n",
      "Step 1210: loss 1.6490358114242554\n",
      "Step 1220: loss 1.6306889057159424\n",
      "Step 1230: loss 1.6311025619506836\n",
      "Step 1240: loss 1.6679613590240479\n",
      "Step 1250: loss 1.6175849437713623\n",
      "Step 1260: loss 1.623594880104065\n",
      "Step 1270: loss 1.5774904489517212\n",
      "Step 1280: loss 1.5582343339920044\n",
      "Step 1290: loss 1.6072214841842651\n",
      "Step 1300: loss 1.5598520040512085\n",
      "--> val loss 1.642885446548462\n",
      "Step 1310: loss 1.5545926094055176\n",
      "Step 1320: loss 1.5846951007843018\n",
      "Step 1330: loss 1.5548291206359863\n",
      "Step 1340: loss 1.5564507246017456\n",
      "Step 1350: loss 1.6087608337402344\n",
      "Step 1360: loss 1.6324971914291382\n",
      "Step 1370: loss 1.5629688501358032\n",
      "Step 1380: loss 1.5720237493515015\n",
      "Step 1390: loss 1.5659358501434326\n",
      "Step 1400: loss 1.5443183183670044\n",
      "--> val loss 1.6145985126495361\n",
      "Step 1410: loss 1.5603277683258057\n",
      "Step 1420: loss 1.5725172758102417\n",
      "Step 1430: loss 1.5623372793197632\n",
      "Step 1440: loss 1.5774791240692139\n",
      "Step 1450: loss 1.593534231185913\n",
      "Step 1460: loss 1.5275967121124268\n",
      "Step 1470: loss 1.5602601766586304\n",
      "Step 1480: loss 1.584723949432373\n",
      "Step 1490: loss 1.5319719314575195\n",
      "Step 1500: loss 1.5755611658096313\n",
      "--> val loss 1.6063449382781982\n",
      "Step 1510: loss 1.5154407024383545\n",
      "Step 1520: loss 1.6004410982131958\n",
      "Step 1530: loss 1.57843816280365\n",
      "Step 1540: loss 1.5402905941009521\n",
      "Step 1550: loss 1.532163381576538\n",
      "Step 1560: loss 1.54276704788208\n",
      "Step 1570: loss 1.492388367652893\n",
      "Step 1580: loss 1.506262183189392\n",
      "Step 1590: loss 1.5720839500427246\n",
      "Step 1600: loss 1.5562161207199097\n",
      "--> val loss 1.5930341482162476\n",
      "Step 1610: loss 1.5039743185043335\n",
      "Step 1620: loss 1.4638946056365967\n",
      "Step 1630: loss 1.4824632406234741\n",
      "Step 1640: loss 1.5277018547058105\n",
      "Step 1650: loss 1.4909543991088867\n",
      "Step 1660: loss 1.5470540523529053\n",
      "Step 1670: loss 1.539850115776062\n",
      "Step 1680: loss 1.506394624710083\n",
      "Step 1690: loss 1.5213141441345215\n",
      "Step 1700: loss 1.5014859437942505\n",
      "--> val loss 1.5617666244506836\n",
      "Step 1710: loss 1.5141246318817139\n",
      "Step 1720: loss 1.4968781471252441\n",
      "Step 1730: loss 1.473183274269104\n",
      "Step 1740: loss 1.4926420450210571\n",
      "Step 1750: loss 1.5327781438827515\n",
      "Step 1760: loss 1.507157802581787\n",
      "Step 1770: loss 1.526289939880371\n",
      "Step 1780: loss 1.4856221675872803\n",
      "Step 1790: loss 1.4858680963516235\n",
      "Step 1800: loss 1.4829984903335571\n",
      "--> val loss 1.509582281112671\n",
      "Step 1810: loss 1.5611395835876465\n",
      "Step 1820: loss 1.4868690967559814\n",
      "Step 1830: loss 1.4754717350006104\n",
      "Step 1840: loss 1.4430712461471558\n",
      "Step 1850: loss 1.4539568424224854\n",
      "Step 1860: loss 1.4825036525726318\n",
      "Step 1870: loss 1.4986557960510254\n",
      "Step 1880: loss 1.491361141204834\n",
      "Step 1890: loss 1.466291904449463\n",
      "Step 1900: loss 1.4426934719085693\n",
      "--> val loss 1.4846473932266235\n",
      "Step 1910: loss 1.467796802520752\n",
      "Step 1920: loss 1.4659160375595093\n",
      "Step 1930: loss 1.4030145406723022\n",
      "Step 1940: loss 1.4543465375900269\n",
      "Step 1950: loss 1.4557034969329834\n",
      "Step 1960: loss 1.4355653524398804\n",
      "Step 1970: loss 1.4682923555374146\n",
      "Step 1980: loss 1.4375624656677246\n",
      "Step 1990: loss 1.4449220895767212\n",
      "Step 2000: loss 1.500573992729187\n",
      "--> val loss 1.474791169166565\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "w = model.initialize(key)\n",
    "\n",
    "step = 0\n",
    "momentum = [0 * weight for weight in w]\n",
    "lr_schedule = lambda step: lr * (steps - step) / steps\n",
    "for inputs, targets in train_loader:\n",
    "    loss, grad_w = loss_and_grad(w, inputs, targets)\n",
    "    momentum = [beta * m + (1 - beta) * g_w for m, g_w in zip(momentum, grad_w)]\n",
    "    d_w = model.dualize(momentum)\n",
    "    w = [weight - lr_schedule(step) * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: loss {loss}\")\n",
    "    \n",
    "    if step % val_interval == 0:\n",
    "        val_losses = []\n",
    "        val_batches = 10\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            loss, _ = loss_and_grad(w, val_inputs, val_targets)\n",
    "            val_losses.append(loss)\n",
    "            if len(val_losses) >= val_batches:\n",
    "                break\n",
    "        print(f\"--> val loss {sum(val_losses)/len(val_losses)}\")\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    if step >= steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Though this be madness, yet there is method in't\n",
    "\n",
    "And indeed, let us look at how our wee model stacks up the master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "\n",
      "If would him to the with this place, the not soul.\n",
      "\n",
      "COMINIUS:\n",
      "I will be make burnt they sours twere my\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 1:\n",
      "\n",
      "If as the consul to good my soldiers,\n",
      "Which tribunes and now the bear to ratter withoughtself ourand t\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 2:\n",
      "\n",
      "If woman, the noight were the life to the earth\n",
      "I mean the cousin be draw his son of the died with tra\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_tokens=100, temperature=0.5, seed=0):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    tokens = jnp.array(encode(prompt))\n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(jnp.expand_dims(tokens, 0), w)\n",
    "        next_token_logits = logits[0, -1] / temperature\n",
    "        \n",
    "        # Sample from our model's token distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        next_token = jax.random.categorical(subkey, next_token_logits)\n",
    "        tokens = jnp.append(tokens, next_token)\n",
    "    \n",
    "    return decode(tokens)\n",
    "\n",
    "for seed in range(3):\n",
    "    print(f\"Sample {seed}:\\n\\n{generate_text('If', max_tokens=100, seed=seed)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, not too bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
