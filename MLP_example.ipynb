{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd286c88-ce33-4be7-8aec-3c3fe5176c40",
   "metadata": {},
   "source": [
    "# MLP Contrained training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842cc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import jax\n",
    "import numpy as np\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "notebook_path = Path(os.getcwd())\n",
    "parent_path = notebook_path.parent / 'min_train_MLP'\n",
    "sys.path.append(str(parent_path))\n",
    "\n",
    "from configs import parse_config_from_json\n",
    "from data_loaders import get_data_loader\n",
    "from models import create_model\n",
    "from optimizers import get_optimizer\n",
    "from trainer import Trainer\n",
    "from utils import Logger, save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87ea7a",
   "metadata": {},
   "source": [
    "Specify some model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ea2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Constrained_MLP_Config = {\n",
    "    'optimizer': 'muon',\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0,\n",
    "    'spectral_wd': 0,\n",
    "    'lr': 0.398,\n",
    "    'output_dim': 10,\n",
    "    'input_dim': 32 * 32 * 3,\n",
    "    'd_embed': 256,\n",
    "    'num_blocks': 3,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'project': {'default': 'soft_cap'},\n",
    "    'w_max': 6,\n",
    "    'sensitive_to_wmax': {'default': True},\n",
    "    'batch_size': 512,\n",
    "    'data': 'cifar',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 98,\n",
    "    'steps': 4900,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': True,\n",
    "    'log_interval': 14,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "Unconstrained_MLP_Config = {\n",
    "    'optimizer': 'adam',\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'wd': 0.08,\n",
    "    'spectral_wd': 0,\n",
    "    'lr': 0.0013,\n",
    "    'output_dim': 10,\n",
    "    'input_dim': 32 * 32 * 3,\n",
    "    'd_embed': 256,\n",
    "    'num_blocks': 3,\n",
    "    'model_dtype': 'float32',\n",
    "    'project_dtype': 'float32',\n",
    "    'zero_init': True,\n",
    "    'project': {'default': 'none'},\n",
    "    'w_max': 1,\n",
    "    'sensitive_to_wmax': {'default': False},\n",
    "    'batch_size': 512,\n",
    "    'data': 'cifar',\n",
    "    'randomize_labels': False,\n",
    "    'val_iters': 20,\n",
    "    'val_interval': 98, \n",
    "    'steps': 4900,\n",
    "    'accum_steps': 1,\n",
    "    'pre_dualize': False,\n",
    "    'post_dualize': False,\n",
    "    'log_interval': 14,\n",
    "    'schedule': 'linear'\n",
    "}\n",
    "\n",
    "constrained_config = parse_config_from_json(Constrained_MLP_Config)\n",
    "unconstrained_config = parse_config_from_json(Unconstrained_MLP_Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc20c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Specify here which config you want to use!!!\n",
    "\n",
    "config = unconstrained_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1194381",
   "metadata": {},
   "source": [
    "Set up experiment and initalize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e305be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "477a30ee-9242-4207-a1f7-c8e4b5c702b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, loss_fn = get_data_loader(config)\n",
    "model = create_model(config)\n",
    "model.jit()\n",
    "optimizer = get_optimizer(config)\n",
    "logger = Logger(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11403d0c",
   "metadata": {},
   "source": [
    "Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e2a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "params = model.initialize(subkey)\n",
    "opt_state = optimizer.init_state(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7935d76",
   "metadata": {},
   "source": [
    "Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ea1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    loss_fn = loss_fn,\n",
    "    config = config,\n",
    "    logger = logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57013ff",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e90277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:10 gpu 1.3G ram 3.8G] Step:14/4900 train_loss:2.1249 train_acc:0.2090 ETA:04:19:07\n",
      "[18:34:10 gpu 1.3G ram 3.8G] Step:28/4900 train_loss:1.9817 train_acc:0.2617 ETA:02:13:53\n",
      "[18:34:10 gpu 1.3G ram 3.8G] Step:42/4900 train_loss:1.9068 train_acc:0.3203 ETA:01:30:12\n",
      "[18:34:10 gpu 1.3G ram 3.8G] Step:56/4900 train_loss:1.8717 train_acc:0.2969 ETA:01:07:59\n",
      "[18:34:10 gpu 1.3G ram 3.8G] Step:70/4900 train_loss:1.8679 train_acc:0.3105 ETA:00:54:31\n",
      "[18:34:11 gpu 1.3G ram 3.8G] Step:84/4900 train_loss:1.8900 train_acc:0.2969 ETA:00:45:29\n",
      "[18:34:11 gpu 1.3G ram 3.8G] Step:98/4900 train_loss:1.7900 train_acc:0.4141 ETA:00:39:02\n",
      "  Step:98/4900 val_loss:1.7866 val_acc:0.3563\n",
      "[18:34:11 gpu 1.3G ram 3.8G] Step:112/4900 train_loss:1.7676 train_acc:0.3516 ETA:00:34:19\n",
      "[18:34:11 gpu 1.3G ram 3.8G] Step:126/4900 train_loss:1.8046 train_acc:0.3594 ETA:00:30:30\n",
      "[18:34:11 gpu 1.3G ram 3.8G] Step:140/4900 train_loss:1.6607 train_acc:0.4102 ETA:00:27:27\n",
      "[18:34:11 gpu 1.3G ram 3.8G] Step:154/4900 train_loss:1.7122 train_acc:0.4004 ETA:00:24:56\n",
      "[18:34:11 gpu 1.3G ram 3.8G] Step:168/4900 train_loss:1.7106 train_acc:0.4004 ETA:00:22:51\n",
      "[18:34:11 gpu 1.3G ram 3.8G] Step:182/4900 train_loss:1.7717 train_acc:0.3652 ETA:00:21:04\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:196/4900 train_loss:1.6180 train_acc:0.4473 ETA:00:19:34\n",
      "  Step:196/4900 val_loss:1.6756 val_acc:0.3992\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:210/4900 train_loss:1.6578 train_acc:0.3867 ETA:00:18:15\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:224/4900 train_loss:1.6010 train_acc:0.4512 ETA:00:17:06\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:238/4900 train_loss:1.6111 train_acc:0.4297 ETA:00:16:05\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:252/4900 train_loss:1.6751 train_acc:0.4043 ETA:00:15:10\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:266/4900 train_loss:1.6886 train_acc:0.4160 ETA:00:14:21\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:280/4900 train_loss:1.6413 train_acc:0.4023 ETA:00:13:38\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:294/4900 train_loss:1.6175 train_acc:0.4102 ETA:00:12:58\n",
      "  Step:294/4900 val_loss:1.6418 val_acc:0.4094\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:308/4900 train_loss:1.6471 train_acc:0.3965 ETA:00:12:23\n",
      "[18:34:12 gpu 1.3G ram 3.8G] Step:322/4900 train_loss:1.5967 train_acc:0.4238 ETA:00:11:50\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:336/4900 train_loss:1.5875 train_acc:0.4277 ETA:00:11:19\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:350/4900 train_loss:1.5763 train_acc:0.4355 ETA:00:10:51\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:364/4900 train_loss:1.5481 train_acc:0.4902 ETA:00:10:25\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:378/4900 train_loss:1.5585 train_acc:0.4648 ETA:00:10:02\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:392/4900 train_loss:1.6086 train_acc:0.4238 ETA:00:09:40\n",
      "  Step:392/4900 val_loss:1.5718 val_acc:0.4462\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:406/4900 train_loss:1.6086 train_acc:0.4395 ETA:00:09:19\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:420/4900 train_loss:1.4754 train_acc:0.4941 ETA:00:09:00\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:434/4900 train_loss:1.5558 train_acc:0.4551 ETA:00:08:42\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:448/4900 train_loss:1.5517 train_acc:0.4668 ETA:00:08:25\n",
      "[18:34:13 gpu 1.3G ram 3.8G] Step:462/4900 train_loss:1.5498 train_acc:0.4570 ETA:00:08:09\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:476/4900 train_loss:1.5695 train_acc:0.4277 ETA:00:07:54\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:490/4900 train_loss:1.5323 train_acc:0.4551 ETA:00:07:40\n",
      "  Step:490/4900 val_loss:1.5224 val_acc:0.4537\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:504/4900 train_loss:1.4539 train_acc:0.4668 ETA:00:07:27\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:518/4900 train_loss:1.4539 train_acc:0.4746 ETA:00:07:14\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:532/4900 train_loss:1.5053 train_acc:0.5117 ETA:00:07:02\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:546/4900 train_loss:1.5298 train_acc:0.4648 ETA:00:06:51\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:560/4900 train_loss:1.4850 train_acc:0.4805 ETA:00:06:40\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:574/4900 train_loss:1.4627 train_acc:0.4570 ETA:00:06:30\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:588/4900 train_loss:1.4408 train_acc:0.5117 ETA:00:06:20\n",
      "  Step:588/4900 val_loss:1.5086 val_acc:0.4642\n",
      "[18:34:14 gpu 1.3G ram 3.8G] Step:602/4900 train_loss:1.4388 train_acc:0.4844 ETA:00:06:11\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:616/4900 train_loss:1.5826 train_acc:0.4355 ETA:00:06:02\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:630/4900 train_loss:1.4497 train_acc:0.4902 ETA:00:05:53\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:644/4900 train_loss:1.4802 train_acc:0.4648 ETA:00:05:45\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:658/4900 train_loss:1.4433 train_acc:0.4980 ETA:00:05:37\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:672/4900 train_loss:1.4551 train_acc:0.4707 ETA:00:05:30\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:686/4900 train_loss:1.4385 train_acc:0.4961 ETA:00:05:23\n",
      "  Step:686/4900 val_loss:1.4801 val_acc:0.4783\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:700/4900 train_loss:1.4259 train_acc:0.4883 ETA:00:05:16\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:714/4900 train_loss:1.4721 train_acc:0.4414 ETA:00:05:09\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:728/4900 train_loss:1.5622 train_acc:0.4688 ETA:00:05:03\n",
      "[18:34:15 gpu 1.3G ram 3.8G] Step:742/4900 train_loss:1.4324 train_acc:0.5352 ETA:00:04:56\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:756/4900 train_loss:1.3629 train_acc:0.5293 ETA:00:04:50\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:770/4900 train_loss:1.4428 train_acc:0.4727 ETA:00:04:45\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:784/4900 train_loss:1.4255 train_acc:0.5117 ETA:00:04:39\n",
      "  Step:784/4900 val_loss:1.4831 val_acc:0.4713\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:798/4900 train_loss:1.4673 train_acc:0.4609 ETA:00:04:34\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:812/4900 train_loss:1.4687 train_acc:0.4805 ETA:00:04:29\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:826/4900 train_loss:1.4412 train_acc:0.4570 ETA:00:04:24\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:840/4900 train_loss:1.4012 train_acc:0.5156 ETA:00:04:19\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:854/4900 train_loss:1.3598 train_acc:0.5527 ETA:00:04:14\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:868/4900 train_loss:1.4464 train_acc:0.5215 ETA:00:04:10\n",
      "[18:34:16 gpu 1.3G ram 3.8G] Step:882/4900 train_loss:1.4773 train_acc:0.5020 ETA:00:04:05\n",
      "  Step:882/4900 val_loss:1.4656 val_acc:0.4750\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:896/4900 train_loss:1.4368 train_acc:0.5000 ETA:00:04:01\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:910/4900 train_loss:1.3350 train_acc:0.5527 ETA:00:03:57\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:924/4900 train_loss:1.4339 train_acc:0.5039 ETA:00:03:53\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:938/4900 train_loss:1.4643 train_acc:0.4902 ETA:00:03:49\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:952/4900 train_loss:1.4449 train_acc:0.4844 ETA:00:03:45\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:966/4900 train_loss:1.3779 train_acc:0.5312 ETA:00:03:42\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:980/4900 train_loss:1.4153 train_acc:0.5000 ETA:00:03:38\n",
      "  Step:980/4900 val_loss:1.4469 val_acc:0.4844\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:994/4900 train_loss:1.3920 train_acc:0.5215 ETA:00:03:35\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:1008/4900 train_loss:1.3886 train_acc:0.5254 ETA:00:03:31\n",
      "[18:34:17 gpu 1.3G ram 3.8G] Step:1022/4900 train_loss:1.3805 train_acc:0.4805 ETA:00:03:28\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1036/4900 train_loss:1.4394 train_acc:0.5215 ETA:00:03:25\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1050/4900 train_loss:1.3767 train_acc:0.5039 ETA:00:03:22\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1064/4900 train_loss:1.3398 train_acc:0.4980 ETA:00:03:19\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1078/4900 train_loss:1.4221 train_acc:0.5020 ETA:00:03:16\n",
      "  Step:1078/4900 val_loss:1.4820 val_acc:0.4716\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1092/4900 train_loss:1.3357 train_acc:0.5684 ETA:00:03:13\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1106/4900 train_loss:1.3384 train_acc:0.5215 ETA:00:03:10\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1120/4900 train_loss:1.3660 train_acc:0.4805 ETA:00:03:07\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1134/4900 train_loss:1.2921 train_acc:0.5371 ETA:00:03:05\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1148/4900 train_loss:1.3720 train_acc:0.5039 ETA:00:03:02\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1162/4900 train_loss:1.3696 train_acc:0.4883 ETA:00:02:59\n",
      "[18:34:18 gpu 1.3G ram 3.8G] Step:1176/4900 train_loss:1.3545 train_acc:0.4824 ETA:00:02:57\n",
      "  Step:1176/4900 val_loss:1.4459 val_acc:0.4830\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1190/4900 train_loss:1.4200 train_acc:0.4902 ETA:00:02:54\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1204/4900 train_loss:1.3907 train_acc:0.5176 ETA:00:02:52\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1218/4900 train_loss:1.2903 train_acc:0.5488 ETA:00:02:50\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1232/4900 train_loss:1.3547 train_acc:0.5273 ETA:00:02:47\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1246/4900 train_loss:1.3179 train_acc:0.5293 ETA:00:02:45\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1260/4900 train_loss:1.3564 train_acc:0.5078 ETA:00:02:43\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1274/4900 train_loss:1.3160 train_acc:0.5410 ETA:00:02:41\n",
      "  Step:1274/4900 val_loss:1.4085 val_acc:0.5040\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1288/4900 train_loss:1.3297 train_acc:0.5332 ETA:00:02:39\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1302/4900 train_loss:1.3535 train_acc:0.5254 ETA:00:02:37\n",
      "[18:34:19 gpu 1.3G ram 3.8G] Step:1316/4900 train_loss:1.3355 train_acc:0.4961 ETA:00:02:35\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1330/4900 train_loss:1.3889 train_acc:0.5312 ETA:00:02:33\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1344/4900 train_loss:1.3425 train_acc:0.5332 ETA:00:02:31\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1358/4900 train_loss:1.3363 train_acc:0.5332 ETA:00:02:29\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1372/4900 train_loss:1.3648 train_acc:0.5156 ETA:00:02:27\n",
      "  Step:1372/4900 val_loss:1.4069 val_acc:0.5005\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1386/4900 train_loss:1.2657 train_acc:0.5664 ETA:00:02:25\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1400/4900 train_loss:1.2801 train_acc:0.5566 ETA:00:02:24\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1414/4900 train_loss:1.3316 train_acc:0.5312 ETA:00:02:22\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1428/4900 train_loss:1.3055 train_acc:0.5508 ETA:00:02:20\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1442/4900 train_loss:1.2523 train_acc:0.5703 ETA:00:02:18\n",
      "[18:34:20 gpu 1.3G ram 3.8G] Step:1456/4900 train_loss:1.3068 train_acc:0.5312 ETA:00:02:17\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1470/4900 train_loss:1.2667 train_acc:0.5703 ETA:00:02:15\n",
      "  Step:1470/4900 val_loss:1.3915 val_acc:0.5028\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1484/4900 train_loss:1.3251 train_acc:0.5156 ETA:00:02:13\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1498/4900 train_loss:1.2602 train_acc:0.5527 ETA:00:02:12\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1512/4900 train_loss:1.3175 train_acc:0.5723 ETA:00:02:10\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1526/4900 train_loss:1.2211 train_acc:0.5742 ETA:00:02:09\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1540/4900 train_loss:1.3311 train_acc:0.5469 ETA:00:02:07\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1554/4900 train_loss:1.3035 train_acc:0.5547 ETA:00:02:06\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1568/4900 train_loss:1.2955 train_acc:0.5176 ETA:00:02:04\n",
      "  Step:1568/4900 val_loss:1.4196 val_acc:0.4997\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1582/4900 train_loss:1.2282 train_acc:0.5918 ETA:00:02:03\n",
      "[18:34:21 gpu 1.3G ram 3.8G] Step:1596/4900 train_loss:1.2460 train_acc:0.5430 ETA:00:02:02\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1610/4900 train_loss:1.2397 train_acc:0.5566 ETA:00:02:00\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1624/4900 train_loss:1.1817 train_acc:0.6211 ETA:00:01:59\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1638/4900 train_loss:1.2118 train_acc:0.5820 ETA:00:01:58\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1652/4900 train_loss:1.1682 train_acc:0.5938 ETA:00:01:56\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1666/4900 train_loss:1.2239 train_acc:0.5684 ETA:00:01:55\n",
      "  Step:1666/4900 val_loss:1.3944 val_acc:0.4995\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1680/4900 train_loss:1.2516 train_acc:0.5547 ETA:00:01:54\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1694/4900 train_loss:1.2523 train_acc:0.5586 ETA:00:01:52\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1708/4900 train_loss:1.2153 train_acc:0.5781 ETA:00:01:51\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1722/4900 train_loss:1.3688 train_acc:0.5527 ETA:00:01:50\n",
      "[18:34:22 gpu 1.3G ram 3.8G] Step:1736/4900 train_loss:1.2994 train_acc:0.5488 ETA:00:01:49\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1750/4900 train_loss:1.2141 train_acc:0.5625 ETA:00:01:48\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1764/4900 train_loss:1.1952 train_acc:0.6055 ETA:00:01:46\n",
      "  Step:1764/4900 val_loss:1.3660 val_acc:0.5160\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1778/4900 train_loss:1.2511 train_acc:0.5352 ETA:00:01:45\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1792/4900 train_loss:1.2538 train_acc:0.5918 ETA:00:01:44\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1806/4900 train_loss:1.2411 train_acc:0.5430 ETA:00:01:43\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1820/4900 train_loss:1.1422 train_acc:0.5918 ETA:00:01:42\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1834/4900 train_loss:1.2090 train_acc:0.5527 ETA:00:01:41\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1848/4900 train_loss:1.2808 train_acc:0.5469 ETA:00:01:40\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1862/4900 train_loss:1.1613 train_acc:0.5898 ETA:00:01:39\n",
      "  Step:1862/4900 val_loss:1.3836 val_acc:0.5118\n",
      "[18:34:23 gpu 1.3G ram 3.8G] Step:1876/4900 train_loss:1.3034 train_acc:0.5391 ETA:00:01:38\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:1890/4900 train_loss:1.1842 train_acc:0.5605 ETA:00:01:37\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:1904/4900 train_loss:1.1953 train_acc:0.5547 ETA:00:01:36\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:1918/4900 train_loss:1.2617 train_acc:0.5898 ETA:00:01:35\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:1932/4900 train_loss:1.2657 train_acc:0.5625 ETA:00:01:34\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:1946/4900 train_loss:1.1870 train_acc:0.5977 ETA:00:01:33\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:1960/4900 train_loss:1.1784 train_acc:0.5781 ETA:00:01:32\n",
      "  Step:1960/4900 val_loss:1.3746 val_acc:0.5166\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:1974/4900 train_loss:1.3616 train_acc:0.5410 ETA:00:01:31\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:1988/4900 train_loss:1.1537 train_acc:0.6016 ETA:00:01:30\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:2002/4900 train_loss:1.1178 train_acc:0.6328 ETA:00:01:29\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:2016/4900 train_loss:1.1934 train_acc:0.5977 ETA:00:01:28\n",
      "[18:34:24 gpu 1.3G ram 3.8G] Step:2030/4900 train_loss:1.2314 train_acc:0.5918 ETA:00:01:27\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2044/4900 train_loss:1.1613 train_acc:0.5742 ETA:00:01:26\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2058/4900 train_loss:1.1762 train_acc:0.5820 ETA:00:01:25\n",
      "  Step:2058/4900 val_loss:1.4022 val_acc:0.5092\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2072/4900 train_loss:1.2348 train_acc:0.5859 ETA:00:01:25\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2086/4900 train_loss:1.1751 train_acc:0.5684 ETA:00:01:24\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2100/4900 train_loss:1.0993 train_acc:0.6055 ETA:00:01:23\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2114/4900 train_loss:1.2698 train_acc:0.5684 ETA:00:01:22\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2128/4900 train_loss:1.2045 train_acc:0.5996 ETA:00:01:21\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2142/4900 train_loss:1.1118 train_acc:0.5938 ETA:00:01:20\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2156/4900 train_loss:1.1316 train_acc:0.6016 ETA:00:01:20\n",
      "  Step:2156/4900 val_loss:1.3598 val_acc:0.5178\n",
      "[18:34:25 gpu 1.3G ram 3.8G] Step:2170/4900 train_loss:1.2149 train_acc:0.5820 ETA:00:01:19\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2184/4900 train_loss:1.1236 train_acc:0.5977 ETA:00:01:18\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2198/4900 train_loss:1.1888 train_acc:0.5938 ETA:00:01:17\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2212/4900 train_loss:1.1671 train_acc:0.6035 ETA:00:01:16\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2226/4900 train_loss:1.2712 train_acc:0.5684 ETA:00:01:16\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2240/4900 train_loss:1.3152 train_acc:0.5430 ETA:00:01:15\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2254/4900 train_loss:1.1535 train_acc:0.6113 ETA:00:01:14\n",
      "  Step:2254/4900 val_loss:1.3747 val_acc:0.5156\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2268/4900 train_loss:1.1128 train_acc:0.6055 ETA:00:01:14\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2282/4900 train_loss:1.1614 train_acc:0.5996 ETA:00:01:13\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2296/4900 train_loss:1.2783 train_acc:0.5723 ETA:00:01:12\n",
      "[18:34:26 gpu 1.3G ram 3.8G] Step:2310/4900 train_loss:1.2351 train_acc:0.5488 ETA:00:01:11\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2324/4900 train_loss:1.2018 train_acc:0.5977 ETA:00:01:11\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2338/4900 train_loss:1.1945 train_acc:0.5605 ETA:00:01:10\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2352/4900 train_loss:1.1484 train_acc:0.5977 ETA:00:01:09\n",
      "  Step:2352/4900 val_loss:1.3524 val_acc:0.5196\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2366/4900 train_loss:1.2266 train_acc:0.5898 ETA:00:01:09\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2380/4900 train_loss:1.1441 train_acc:0.5996 ETA:00:01:08\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2394/4900 train_loss:1.0827 train_acc:0.6250 ETA:00:01:07\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2408/4900 train_loss:1.0704 train_acc:0.6387 ETA:00:01:06\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2422/4900 train_loss:1.2219 train_acc:0.5801 ETA:00:01:06\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2436/4900 train_loss:1.1321 train_acc:0.6074 ETA:00:01:05\n",
      "[18:34:27 gpu 1.3G ram 3.8G] Step:2450/4900 train_loss:1.0706 train_acc:0.6230 ETA:00:01:05\n",
      "  Step:2450/4900 val_loss:1.3525 val_acc:0.5250\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2464/4900 train_loss:1.2214 train_acc:0.5996 ETA:00:01:04\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2478/4900 train_loss:1.2009 train_acc:0.6055 ETA:00:01:03\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2492/4900 train_loss:1.1494 train_acc:0.6289 ETA:00:01:03\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2506/4900 train_loss:1.1720 train_acc:0.6035 ETA:00:01:02\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2520/4900 train_loss:1.1836 train_acc:0.5723 ETA:00:01:01\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2534/4900 train_loss:1.2603 train_acc:0.5645 ETA:00:01:01\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2548/4900 train_loss:1.0734 train_acc:0.6172 ETA:00:01:00\n",
      "  Step:2548/4900 val_loss:1.3507 val_acc:0.5261\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2562/4900 train_loss:1.0371 train_acc:0.6367 ETA:00:01:00\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2576/4900 train_loss:1.0889 train_acc:0.6406 ETA:00:00:59\n",
      "[18:34:28 gpu 1.3G ram 3.8G] Step:2590/4900 train_loss:1.1261 train_acc:0.6172 ETA:00:00:58\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2604/4900 train_loss:1.1068 train_acc:0.6094 ETA:00:00:58\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2618/4900 train_loss:1.0820 train_acc:0.6387 ETA:00:00:57\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2632/4900 train_loss:1.1667 train_acc:0.6074 ETA:00:00:57\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2646/4900 train_loss:1.0810 train_acc:0.6309 ETA:00:00:56\n",
      "  Step:2646/4900 val_loss:1.3510 val_acc:0.5269\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2660/4900 train_loss:1.1805 train_acc:0.5703 ETA:00:00:56\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2674/4900 train_loss:1.1415 train_acc:0.6230 ETA:00:00:55\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2688/4900 train_loss:1.2006 train_acc:0.5898 ETA:00:00:54\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2702/4900 train_loss:1.0852 train_acc:0.6270 ETA:00:00:54\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2716/4900 train_loss:1.1838 train_acc:0.5879 ETA:00:00:53\n",
      "[18:34:29 gpu 1.3G ram 3.8G] Step:2730/4900 train_loss:1.0448 train_acc:0.6465 ETA:00:00:53\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2744/4900 train_loss:1.1135 train_acc:0.6191 ETA:00:00:52\n",
      "  Step:2744/4900 val_loss:1.3524 val_acc:0.5234\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2758/4900 train_loss:1.1813 train_acc:0.5762 ETA:00:00:52\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2772/4900 train_loss:1.1614 train_acc:0.5781 ETA:00:00:51\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2786/4900 train_loss:1.0690 train_acc:0.6211 ETA:00:00:51\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2800/4900 train_loss:1.1284 train_acc:0.6270 ETA:00:00:50\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2814/4900 train_loss:1.0832 train_acc:0.6172 ETA:00:00:50\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2828/4900 train_loss:1.1432 train_acc:0.6113 ETA:00:00:49\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2842/4900 train_loss:1.1317 train_acc:0.5820 ETA:00:00:49\n",
      "  Step:2842/4900 val_loss:1.3534 val_acc:0.5249\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2856/4900 train_loss:1.0686 train_acc:0.6426 ETA:00:00:48\n",
      "[18:34:30 gpu 1.3G ram 3.8G] Step:2870/4900 train_loss:1.0751 train_acc:0.6484 ETA:00:00:48\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:2884/4900 train_loss:1.0941 train_acc:0.6230 ETA:00:00:47\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:2898/4900 train_loss:1.0150 train_acc:0.6621 ETA:00:00:47\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:2912/4900 train_loss:1.0678 train_acc:0.6426 ETA:00:00:46\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:2926/4900 train_loss:1.1288 train_acc:0.6289 ETA:00:00:46\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:2940/4900 train_loss:1.1388 train_acc:0.5879 ETA:00:00:45\n",
      "  Step:2940/4900 val_loss:1.3430 val_acc:0.5298\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:2954/4900 train_loss:1.0565 train_acc:0.6484 ETA:00:00:45\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:2968/4900 train_loss:1.1489 train_acc:0.5918 ETA:00:00:44\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:2982/4900 train_loss:1.1592 train_acc:0.5859 ETA:00:00:44\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:2996/4900 train_loss:1.0851 train_acc:0.6211 ETA:00:00:43\n",
      "[18:34:31 gpu 1.3G ram 3.8G] Step:3010/4900 train_loss:1.1587 train_acc:0.6074 ETA:00:00:43\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3024/4900 train_loss:1.1376 train_acc:0.5957 ETA:00:00:42\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3038/4900 train_loss:1.1250 train_acc:0.5996 ETA:00:00:42\n",
      "  Step:3038/4900 val_loss:1.3584 val_acc:0.5235\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3052/4900 train_loss:1.1360 train_acc:0.5859 ETA:00:00:41\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3066/4900 train_loss:1.0515 train_acc:0.6367 ETA:00:00:41\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3080/4900 train_loss:1.0676 train_acc:0.6289 ETA:00:00:41\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3094/4900 train_loss:1.0698 train_acc:0.6367 ETA:00:00:40\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3108/4900 train_loss:1.0478 train_acc:0.6445 ETA:00:00:40\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3122/4900 train_loss:1.0250 train_acc:0.6289 ETA:00:00:39\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3136/4900 train_loss:1.0217 train_acc:0.6172 ETA:00:00:39\n",
      "  Step:3136/4900 val_loss:1.3481 val_acc:0.5277\n",
      "[18:34:32 gpu 1.3G ram 3.8G] Step:3150/4900 train_loss:1.0467 train_acc:0.6445 ETA:00:00:38\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3164/4900 train_loss:1.0416 train_acc:0.6309 ETA:00:00:38\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3178/4900 train_loss:0.9957 train_acc:0.6191 ETA:00:00:38\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3192/4900 train_loss:1.0647 train_acc:0.6035 ETA:00:00:37\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3206/4900 train_loss:1.0666 train_acc:0.5918 ETA:00:00:37\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3220/4900 train_loss:1.1131 train_acc:0.6328 ETA:00:00:36\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3234/4900 train_loss:1.0244 train_acc:0.6445 ETA:00:00:36\n",
      "  Step:3234/4900 val_loss:1.3369 val_acc:0.5357\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3248/4900 train_loss:1.0907 train_acc:0.6406 ETA:00:00:35\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3262/4900 train_loss:1.0837 train_acc:0.6289 ETA:00:00:35\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3276/4900 train_loss:1.1606 train_acc:0.6113 ETA:00:00:35\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3290/4900 train_loss:1.0404 train_acc:0.6289 ETA:00:00:34\n",
      "[18:34:33 gpu 1.3G ram 3.8G] Step:3304/4900 train_loss:1.0422 train_acc:0.6387 ETA:00:00:34\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3318/4900 train_loss:1.0832 train_acc:0.6289 ETA:00:00:33\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3332/4900 train_loss:1.1119 train_acc:0.6133 ETA:00:00:33\n",
      "  Step:3332/4900 val_loss:1.3489 val_acc:0.5294\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3346/4900 train_loss:1.0783 train_acc:0.6270 ETA:00:00:33\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3360/4900 train_loss:1.1322 train_acc:0.6016 ETA:00:00:32\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3374/4900 train_loss:1.0831 train_acc:0.6309 ETA:00:00:32\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3388/4900 train_loss:1.1787 train_acc:0.6035 ETA:00:00:31\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3402/4900 train_loss:1.0121 train_acc:0.6367 ETA:00:00:31\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3416/4900 train_loss:1.0414 train_acc:0.6387 ETA:00:00:31\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3430/4900 train_loss:1.0642 train_acc:0.6484 ETA:00:00:30\n",
      "  Step:3430/4900 val_loss:1.3433 val_acc:0.5285\n",
      "[18:34:34 gpu 1.3G ram 3.8G] Step:3444/4900 train_loss:1.0068 train_acc:0.6504 ETA:00:00:30\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3458/4900 train_loss:1.1034 train_acc:0.6016 ETA:00:00:30\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3472/4900 train_loss:0.9899 train_acc:0.6523 ETA:00:00:29\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3486/4900 train_loss:0.9986 train_acc:0.6641 ETA:00:00:29\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3500/4900 train_loss:1.0231 train_acc:0.6465 ETA:00:00:28\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3514/4900 train_loss:1.0698 train_acc:0.6582 ETA:00:00:28\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3528/4900 train_loss:0.9836 train_acc:0.6543 ETA:00:00:28\n",
      "  Step:3528/4900 val_loss:1.3532 val_acc:0.5276\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3542/4900 train_loss:1.0493 train_acc:0.6230 ETA:00:00:27\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3556/4900 train_loss:0.9758 train_acc:0.6816 ETA:00:00:27\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3570/4900 train_loss:1.0076 train_acc:0.6367 ETA:00:00:27\n",
      "[18:34:35 gpu 1.3G ram 3.8G] Step:3584/4900 train_loss:1.1280 train_acc:0.6172 ETA:00:00:26\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3598/4900 train_loss:1.0614 train_acc:0.6172 ETA:00:00:26\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3612/4900 train_loss:1.0383 train_acc:0.6230 ETA:00:00:26\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3626/4900 train_loss:1.0276 train_acc:0.6348 ETA:00:00:25\n",
      "  Step:3626/4900 val_loss:1.3408 val_acc:0.5333\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3640/4900 train_loss:0.9606 train_acc:0.6562 ETA:00:00:25\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3654/4900 train_loss:1.1115 train_acc:0.5879 ETA:00:00:25\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3668/4900 train_loss:1.0339 train_acc:0.6562 ETA:00:00:24\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3682/4900 train_loss:1.0843 train_acc:0.6250 ETA:00:00:24\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3696/4900 train_loss:1.0583 train_acc:0.6211 ETA:00:00:24\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3710/4900 train_loss:1.0659 train_acc:0.6289 ETA:00:00:23\n",
      "[18:34:36 gpu 1.3G ram 3.8G] Step:3724/4900 train_loss:1.0221 train_acc:0.6621 ETA:00:00:23\n",
      "  Step:3724/4900 val_loss:1.3452 val_acc:0.5279\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3738/4900 train_loss:0.9972 train_acc:0.6582 ETA:00:00:23\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3752/4900 train_loss:1.0502 train_acc:0.6465 ETA:00:00:22\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3766/4900 train_loss:1.0308 train_acc:0.6406 ETA:00:00:22\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3780/4900 train_loss:0.9527 train_acc:0.6758 ETA:00:00:22\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3794/4900 train_loss:1.0207 train_acc:0.6445 ETA:00:00:21\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3808/4900 train_loss:0.9904 train_acc:0.6543 ETA:00:00:21\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3822/4900 train_loss:1.0427 train_acc:0.6230 ETA:00:00:21\n",
      "  Step:3822/4900 val_loss:1.3503 val_acc:0.5318\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3836/4900 train_loss:0.9703 train_acc:0.6367 ETA:00:00:20\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3850/4900 train_loss:1.0738 train_acc:0.6035 ETA:00:00:20\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3864/4900 train_loss:1.0154 train_acc:0.6484 ETA:00:00:20\n",
      "[18:34:37 gpu 1.3G ram 3.8G] Step:3878/4900 train_loss:1.0660 train_acc:0.6562 ETA:00:00:19\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:3892/4900 train_loss:1.0684 train_acc:0.6562 ETA:00:00:19\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:3906/4900 train_loss:0.9800 train_acc:0.6523 ETA:00:00:19\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:3920/4900 train_loss:0.9350 train_acc:0.6758 ETA:00:00:18\n",
      "  Step:3920/4900 val_loss:1.3396 val_acc:0.5342\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:3934/4900 train_loss:1.0761 train_acc:0.6270 ETA:00:00:18\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:3948/4900 train_loss:0.9533 train_acc:0.6855 ETA:00:00:18\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:3962/4900 train_loss:0.9668 train_acc:0.6719 ETA:00:00:17\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:3976/4900 train_loss:1.0189 train_acc:0.6484 ETA:00:00:17\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:3990/4900 train_loss:1.0481 train_acc:0.6660 ETA:00:00:17\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:4004/4900 train_loss:0.9526 train_acc:0.6953 ETA:00:00:16\n",
      "[18:34:38 gpu 1.3G ram 3.8G] Step:4018/4900 train_loss:0.8941 train_acc:0.6895 ETA:00:00:16\n",
      "  Step:4018/4900 val_loss:1.3462 val_acc:0.5319\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4032/4900 train_loss:1.0081 train_acc:0.6523 ETA:00:00:16\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4046/4900 train_loss:1.0007 train_acc:0.6270 ETA:00:00:16\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4060/4900 train_loss:1.0574 train_acc:0.6602 ETA:00:00:15\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4074/4900 train_loss:0.9956 train_acc:0.6602 ETA:00:00:15\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4088/4900 train_loss:0.9765 train_acc:0.6641 ETA:00:00:15\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4102/4900 train_loss:0.9287 train_acc:0.7012 ETA:00:00:14\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4116/4900 train_loss:1.0162 train_acc:0.6426 ETA:00:00:14\n",
      "  Step:4116/4900 val_loss:1.3494 val_acc:0.5313\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4130/4900 train_loss:0.9896 train_acc:0.6523 ETA:00:00:14\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4144/4900 train_loss:0.9640 train_acc:0.6797 ETA:00:00:14\n",
      "[18:34:39 gpu 1.3G ram 3.8G] Step:4158/4900 train_loss:1.0088 train_acc:0.6406 ETA:00:00:13\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4172/4900 train_loss:1.0456 train_acc:0.6523 ETA:00:00:13\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4186/4900 train_loss:1.0094 train_acc:0.6445 ETA:00:00:13\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4200/4900 train_loss:0.9749 train_acc:0.6602 ETA:00:00:12\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4214/4900 train_loss:0.9525 train_acc:0.7051 ETA:00:00:12\n",
      "  Step:4214/4900 val_loss:1.3521 val_acc:0.5352\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4228/4900 train_loss:0.9741 train_acc:0.6738 ETA:00:00:12\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4242/4900 train_loss:0.9707 train_acc:0.6562 ETA:00:00:12\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4256/4900 train_loss:0.9674 train_acc:0.6660 ETA:00:00:11\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4270/4900 train_loss:0.9474 train_acc:0.6914 ETA:00:00:11\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4284/4900 train_loss:0.9393 train_acc:0.6914 ETA:00:00:11\n",
      "[18:34:40 gpu 1.3G ram 3.8G] Step:4298/4900 train_loss:0.9134 train_acc:0.6816 ETA:00:00:10\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4312/4900 train_loss:0.9995 train_acc:0.6680 ETA:00:00:10\n",
      "  Step:4312/4900 val_loss:1.3538 val_acc:0.5362\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4326/4900 train_loss:0.9266 train_acc:0.6680 ETA:00:00:10\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4340/4900 train_loss:0.9228 train_acc:0.6816 ETA:00:00:10\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4354/4900 train_loss:0.9790 train_acc:0.6641 ETA:00:00:09\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4368/4900 train_loss:0.9804 train_acc:0.6465 ETA:00:00:09\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4382/4900 train_loss:1.0133 train_acc:0.6504 ETA:00:00:09\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4396/4900 train_loss:0.9334 train_acc:0.6719 ETA:00:00:09\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4410/4900 train_loss:1.0123 train_acc:0.6523 ETA:00:00:08\n",
      "  Step:4410/4900 val_loss:1.3405 val_acc:0.5363\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4424/4900 train_loss:1.0338 train_acc:0.6680 ETA:00:00:08\n",
      "[18:34:41 gpu 1.3G ram 3.8G] Step:4438/4900 train_loss:0.9673 train_acc:0.6270 ETA:00:00:08\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4452/4900 train_loss:0.9532 train_acc:0.6621 ETA:00:00:07\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4466/4900 train_loss:0.9289 train_acc:0.6914 ETA:00:00:07\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4480/4900 train_loss:0.9355 train_acc:0.6738 ETA:00:00:07\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4494/4900 train_loss:0.8852 train_acc:0.6914 ETA:00:00:07\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4508/4900 train_loss:0.9470 train_acc:0.6582 ETA:00:00:06\n",
      "  Step:4508/4900 val_loss:1.3400 val_acc:0.5402\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4522/4900 train_loss:0.9655 train_acc:0.6621 ETA:00:00:06\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4536/4900 train_loss:0.9371 train_acc:0.6758 ETA:00:00:06\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4550/4900 train_loss:0.9971 train_acc:0.6484 ETA:00:00:06\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4564/4900 train_loss:0.9971 train_acc:0.6523 ETA:00:00:05\n",
      "[18:34:42 gpu 1.3G ram 3.8G] Step:4578/4900 train_loss:0.9151 train_acc:0.6836 ETA:00:00:05\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4592/4900 train_loss:0.9668 train_acc:0.6621 ETA:00:00:05\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4606/4900 train_loss:0.9140 train_acc:0.6621 ETA:00:00:05\n",
      "  Step:4606/4900 val_loss:1.3374 val_acc:0.5386\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4620/4900 train_loss:1.0230 train_acc:0.6465 ETA:00:00:04\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4634/4900 train_loss:1.0123 train_acc:0.6504 ETA:00:00:04\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4648/4900 train_loss:0.9120 train_acc:0.7129 ETA:00:00:04\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4662/4900 train_loss:0.9304 train_acc:0.6953 ETA:00:00:04\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4676/4900 train_loss:0.9156 train_acc:0.6895 ETA:00:00:03\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4690/4900 train_loss:0.9050 train_acc:0.6855 ETA:00:00:03\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4704/4900 train_loss:0.9055 train_acc:0.7031 ETA:00:00:03\n",
      "  Step:4704/4900 val_loss:1.3413 val_acc:0.5387\n",
      "[18:34:43 gpu 1.3G ram 3.8G] Step:4718/4900 train_loss:0.9026 train_acc:0.6953 ETA:00:00:03\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4732/4900 train_loss:0.9615 train_acc:0.6699 ETA:00:00:02\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4746/4900 train_loss:0.9262 train_acc:0.6836 ETA:00:00:02\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4760/4900 train_loss:0.9466 train_acc:0.6641 ETA:00:00:02\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4774/4900 train_loss:0.9520 train_acc:0.6719 ETA:00:00:02\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4788/4900 train_loss:0.9385 train_acc:0.6504 ETA:00:00:01\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4802/4900 train_loss:0.9993 train_acc:0.6445 ETA:00:00:01\n",
      "  Step:4802/4900 val_loss:1.3383 val_acc:0.5385\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4816/4900 train_loss:0.9728 train_acc:0.6602 ETA:00:00:01\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4830/4900 train_loss:1.0050 train_acc:0.6641 ETA:00:00:01\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4844/4900 train_loss:0.9708 train_acc:0.6602 ETA:00:00:00\n",
      "[18:34:44 gpu 1.3G ram 3.8G] Step:4858/4900 train_loss:0.8636 train_acc:0.7578 ETA:00:00:00\n",
      "[18:34:45 gpu 1.3G ram 3.8G] Step:4872/4900 train_loss:1.0538 train_acc:0.6328 ETA:00:00:00\n",
      "[18:34:45 gpu 1.3G ram 3.8G] Step:4886/4900 train_loss:0.9266 train_acc:0.6680 ETA:00:00:00\n",
      "[18:34:45 gpu 1.3G ram 3.8G] Step:4900/4900 train_loss:0.9566 train_acc:0.6758 ETA:00:00:00\n",
      "  Step:4900/4900 val_loss:1.3357 val_acc:0.5402\n"
     ]
    }
   ],
   "source": [
    "params, opt_state, key = trainer.train(params, opt_state, key)\n",
    "\n",
    "results = logger.get_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
