{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a transformer with Modula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up the model parameters and the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karpathy's smallest GPT config\n",
    "\n",
    "vocab_size = 65\n",
    "context = 64\n",
    "num_heads = 4\n",
    "d_embed = 128\n",
    "d_query = 32\n",
    "d_value = 32\n",
    "num_blocks = 4\n",
    "attention_scale = 1\n",
    "final_scale = 1\n",
    "\n",
    "# training hyperparameters\n",
    "\n",
    "lr = 0.1\n",
    "beta = 0.95\n",
    "batch_size = 64\n",
    "steps = 201\n",
    "log_interval = 10\n",
    "val_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.shakespeare import load_shakespeare\n",
    "\n",
    "data = load_shakespeare(context, batch_size)\n",
    "\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"val_loader\"]\n",
    "encode = data[\"encode\"]\n",
    "decode = data[\"decode\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 64)\n",
      "Target shape: (64, 64)\n",
      "First input sequence: [41 53 50 42  1 40 50 53 53 42  1 52 53  1 57 54 39 56 49  1 53 44  1 46\n",
      " 53 52 53 59 56  1 40 47 42 43 57  8  0  0 26 27 30 32 20 33 25 14 17 30\n",
      " 24 13 26 16 10  0 14 43  1 58 46 53 59  1 39  1]\n",
      "First target sequence: [53 50 42  1 40 50 53 53 42  1 52 53  1 57 54 39 56 49  1 53 44  1 46 53\n",
      " 52 53 59 56  1 40 47 42 43 57  8  0  0 26 27 30 32 20 33 25 14 17 30 24\n",
      " 13 26 16 10  0 14 43  1 58 46 53 59  1 39  1 54]\n",
      "Decoded input: cold blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a \n",
      "Decoded target: old blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a p\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    print(\"First input sequence:\", inputs[0])\n",
    "    print(\"First target sequence:\", targets[0])\n",
    "    print(\"Decoded input:\", decode(inputs[0]))\n",
    "    print(\"Decoded target:\", decode(targets[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to define our transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 8 atoms and 21 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 7.0 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "from modula.compound import GPT\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    d_embed=d_embed,\n",
    "    d_query=d_query,\n",
    "    d_value=d_value,\n",
    "    num_blocks=num_blocks,\n",
    "    attention_scale=attention_scale,\n",
    "    final_scale=final_scale,\n",
    ")\n",
    "\n",
    "model.jit()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use cross entropy loss. We compute it using the logsumexp trick.\n",
    "\n",
    "$$\n",
    "-\\log(\\text{target probability}) = -\\log(\\text{softmax}(\\text{logits}, \\text{axis}=-1)_\\text{target}) = -\\text{logit}_\\text{target} + \\text{logsumexp}(\\text{logits})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(w, inputs, targets):\n",
    "    # We use the logsumexp trick for stable cross entropy\n",
    "    logits = model(inputs, w)  # shape is [batch, seq_len, vocab_size]\n",
    "    batch_indices = jnp.arange(logits.shape[0])[:, None]  # shape is [batch, 1]\n",
    "    seq_indices = jnp.arange(logits.shape[1])[None, :]    # shape is [1, seq_len]\n",
    "    losses = -logits[batch_indices, seq_indices, targets] + jax.nn.logsumexp(logits, axis=-1)  # shape is [batch, seq_len]\n",
    "    return losses.mean()\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(cross_entropy_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 4.199942588806152\n",
      "Step 10: loss 3.719193935394287\n",
      "Step 20: loss 3.0242934226989746\n",
      "Step 30: loss 2.740544080734253\n",
      "Step 40: loss 2.5253493785858154\n",
      "Step 50: loss 2.4749441146850586\n",
      "Step 60: loss 2.449570894241333\n",
      "Step 70: loss 2.3566646575927734\n",
      "Step 80: loss 2.260190963745117\n",
      "Step 90: loss 2.2548327445983887\n",
      "Step 100: loss 2.191324234008789\n",
      "Step 110: loss 2.1397671699523926\n",
      "Step 120: loss 2.1344094276428223\n",
      "Step 130: loss 2.1281919479370117\n",
      "Step 140: loss 2.0632476806640625\n",
      "Step 150: loss 2.0342979431152344\n",
      "Step 160: loss 2.1126303672790527\n",
      "Step 170: loss 2.009800672531128\n",
      "Step 180: loss 2.022756338119507\n",
      "Step 190: loss 1.9923937320709229\n",
      "Step 200: loss 2.0251433849334717\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "w = model.initialize(key)\n",
    "\n",
    "step = 0\n",
    "momentum = [0 * weight for weight in w]\n",
    "lr_schedule = lambda step: lr * (steps - step) / steps\n",
    "for inputs, targets in train_loader:\n",
    "    loss, grad_w = loss_and_grad(w, inputs, targets)\n",
    "    momentum = [beta * m + (1 - beta) * g_w for m, g_w in zip(momentum, grad_w)]\n",
    "    d_w = model.dualize(momentum)\n",
    "    w = [weight - lr_schedule(step) * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: loss {loss}\")\n",
    "    \n",
    "    # if step % val_interval == 0:\n",
    "    #     val_losses = []\n",
    "    #     for val_inputs, val_targets in val_loader:\n",
    "    #         loss, _ = loss_and_grad(w, val_inputs, val_targets)\n",
    "    #         val_losses.append(loss)\n",
    "    #     print(f\"\\tval loss {sum(val_losses)/len(val_losses)}\")\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    if step >= steps:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
