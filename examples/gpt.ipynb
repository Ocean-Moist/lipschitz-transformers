{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a transformer with Modula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up the model parameters and the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karpathy's smallest GPT config\n",
    "\n",
    "vocab_size = 65\n",
    "context = 64\n",
    "num_heads = 4\n",
    "d_embed = 128\n",
    "d_query = 32\n",
    "d_value = 32\n",
    "num_blocks = 3\n",
    "softmax_scale = 1\n",
    "\n",
    "# training hyperparameters\n",
    "\n",
    "lr = 0.1\n",
    "beta = 0.95\n",
    "batch_size = 64\n",
    "steps = 2001\n",
    "log_interval = 10\n",
    "val_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.shakespeare import load_shakespeare\n",
    "\n",
    "data = load_shakespeare(context, batch_size)\n",
    "\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"val_loader\"]\n",
    "encode = data[\"encode\"]\n",
    "decode = data[\"decode\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 64)\n",
      "Target shape: (64, 64)\n",
      "First input sequence: [41 53 50 42  1 40 50 53 53 42  1 52 53  1 57 54 39 56 49  1 53 44  1 46\n",
      " 53 52 53 59 56  1 40 47 42 43 57  8  0  0 26 27 30 32 20 33 25 14 17 30\n",
      " 24 13 26 16 10  0 14 43  1 58 46 53 59  1 39  1]\n",
      "First target sequence: [53 50 42  1 40 50 53 53 42  1 52 53  1 57 54 39 56 49  1 53 44  1 46 53\n",
      " 52 53 59 56  1 40 47 42 43 57  8  0  0 26 27 30 32 20 33 25 14 17 30 24\n",
      " 13 26 16 10  0 14 43  1 58 46 53 59  1 39  1 54]\n",
      "Decoded input: cold blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a \n",
      "Decoded target: old blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a p\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    print(\"First input sequence:\", inputs[0])\n",
    "    print(\"First target sequence:\", targets[0])\n",
    "    print(\"Decoded input:\", decode(inputs[0]))\n",
    "    print(\"Decoded target:\", decode(targets[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to define our transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 20 atoms and 58 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 7.0 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "from modula.compound import GPT\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    d_embed=d_embed,\n",
    "    d_query=d_query,\n",
    "    d_value=d_value,\n",
    "    num_blocks=num_blocks,\n",
    "    softmax_scale=softmax_scale\n",
    ")\n",
    "\n",
    "model.jit()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use cross entropy loss. We compute it using the logsumexp trick.\n",
    "\n",
    "$$\n",
    "-\\log(\\text{target probability}) = -\\log(\\text{softmax}(\\text{logits}, \\text{axis}=-1)_\\text{target}) = -\\text{logit}_\\text{target} + \\text{logsumexp}(\\text{logits})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(w, inputs, targets):\n",
    "    # We use the logsumexp trick for stable cross entropy\n",
    "    logits = model(inputs, w)  # shape is [batch, seq_len, vocab_size]\n",
    "    batch_indices = jnp.arange(logits.shape[0])[:, None]  # shape is [batch, 1]\n",
    "    seq_indices = jnp.arange(logits.shape[1])[None, :]    # shape is [1, seq_len]\n",
    "    losses = -logits[batch_indices, seq_indices, targets] + jax.nn.logsumexp(logits, axis=-1)  # shape is [batch, seq_len]\n",
    "    return losses.mean()\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(cross_entropy_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 4.184149265289307\n",
      "Step 10: loss 3.728048324584961\n",
      "Step 20: loss 3.069167137145996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     loss, grad_w \u001b[38;5;241m=\u001b[39m loss_and_grad(w, inputs, targets)\n\u001b[0;32m----> 9\u001b[0m     momentum \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mg_w\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_w\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_w\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m     d_w \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdualize(momentum)\n\u001b[1;32m     11\u001b[0m     w \u001b[38;5;241m=\u001b[39m [weight \u001b[38;5;241m-\u001b[39m lr_schedule(step) \u001b[38;5;241m*\u001b[39m d_weight \u001b[38;5;28;01mfor\u001b[39;00m weight, d_weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(w, d_w)]\n",
      "Cell \u001b[0;32mIn[29], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     loss, grad_w \u001b[38;5;241m=\u001b[39m loss_and_grad(w, inputs, targets)\n\u001b[0;32m----> 9\u001b[0m     momentum \u001b[38;5;241m=\u001b[39m [beta \u001b[38;5;241m*\u001b[39m m \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mg_w\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m, g_w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(momentum, grad_w)]\n\u001b[1;32m     10\u001b[0m     d_w \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdualize(momentum)\n\u001b[1;32m     11\u001b[0m     w \u001b[38;5;241m=\u001b[39m [weight \u001b[38;5;241m-\u001b[39m lr_schedule(step) \u001b[38;5;241m*\u001b[39m d_weight \u001b[38;5;28;01mfor\u001b[39;00m weight, d_weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(w, d_w)]\n",
      "File \u001b[0;32m~/Downloads/MEng/spring2025/rnn/.rnn/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:578\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    576\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 578\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/Downloads/MEng/spring2025/rnn/.rnn/lib/python3.11/site-packages/jax/_src/numpy/ufunc_api.py:179\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "w = model.initialize(key)\n",
    "\n",
    "step = 0\n",
    "momentum = [0 * weight for weight in w]\n",
    "lr_schedule = lambda step: lr * (steps - step) / steps\n",
    "for inputs, targets in train_loader:\n",
    "    loss, grad_w = loss_and_grad(w, inputs, targets)\n",
    "    momentum = [beta * m + (1 - beta) * g_w for m, g_w in zip(momentum, grad_w)]\n",
    "    d_w = model.dualize(momentum)\n",
    "    w = [weight - lr_schedule(step) * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: loss {loss}\")\n",
    "    \n",
    "    # if step % val_interval == 0:\n",
    "    #     val_losses = []\n",
    "    #     for val_inputs, val_targets in val_loader:\n",
    "    #         loss, _ = loss_and_grad(w, val_inputs, val_targets)\n",
    "    #         val_losses.append(loss)\n",
    "    #     print(f\"\\tval loss {sum(val_losses)/len(val_losses)}\")\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    if step >= steps:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
