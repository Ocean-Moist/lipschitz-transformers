{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Muon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to train a transformer with precise control over the size of the weights.\n",
    "\n",
    "We'll see how manifold-constrained optimization will let us control numerical properties during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the Shakespeare dataset. The task will be to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 64\n",
    "batch_size = 12\n",
    "\n",
    "from data.shakespeare import load_shakespeare\n",
    "\n",
    "data = load_shakespeare(context, batch_size)\n",
    "\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"val_loader\"]\n",
    "encode = data[\"encode\"]\n",
    "decode = data[\"decode\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at an example to verify the data loaded correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (12, 64)\n",
      "Target shape: (12, 64)\n",
      "First input sequence: [41 53 50 42  1 40 50 53 53 42] ...\n",
      "First target sequence: [53 50 42  1 40 50 53 53 42  1] ...\n",
      "\n",
      "Decoded input: cold blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a \n",
      "\n",
      "Decoded target: old blood no spark of honour bides.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Be thou a p\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    print(\"First input sequence:\", inputs[0][:10], \"...\")\n",
    "    print(\"First target sequence:\", targets[0][:10], \"...\")\n",
    "    print(\"\\nDecoded input:\", decode(inputs[0]))\n",
    "    print(\"\\nDecoded target:\", decode(targets[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the architecture\n",
    "\n",
    "Let's use a very small setting for our transformer so it is fast to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer hyperparameters\n",
    "\n",
    "vocab_size = 65\n",
    "num_heads = 4\n",
    "d_embed = 128\n",
    "num_blocks = 4\n",
    "attention_scale = 1\n",
    "final_scale = 1\n",
    "\n",
    "# training hyperparameters\n",
    "\n",
    "lr = 0.1\n",
    "wd = 0\n",
    "beta = 0.95\n",
    "steps = 2001\n",
    "log_interval = 10\n",
    "val_interval = 100\n",
    "val_iters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next up, we'll define the *attention* module and *residual blocks*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention in Modula\n",
    "\n",
    "In Modula, we'll define attention by stringing together several bond modules to do the parameterless computations. The roadmap is:\n",
    "* Map `(batch, token, d_embed)` into `(batch, head, token, d_query)` (and same for key and value) via `HeadedLinear` and `TransposeHeads`\n",
    "* Use Rotary Positional Embeddings (RoPE) on the query and the key via `Rope`\n",
    "* Map `query` and `key` into attention similarities of shape `(batch, head, token, token)` via `AttentionQK`\n",
    "* Use a causal mask and then softmax to create attention scores via `CausalMask` and `Softmax`\n",
    "* Use the attention scores to create output vectors via `ApplyAttentionScores`, then `TransposeHeads` and `HeadedLinearOut`\n",
    "\n",
    "The main difference to a standard transformer is that `AttentionQK` uses $1/d_\\text{head}$ scaling instead of the standard $1/\\sqrt{d_\\text{head}}$. The reason for this is to provide Lipschitz guarantees for attention that are independent of $d_\\text{head}$. For more information on this, see Appendix B.6 of [Scalable Optimization in the Modular Norm](https://arxiv.org/pdf/2405.14813).\n",
    "\n",
    "And here's the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modula.atom import HeadedLinear, HeadedLinearOut, Scalar\n",
    "from modula.bond import TransposeHeads, ReduceHeads, Rope, AttentionQK, CausalMask, Softmax, ApplyAttentionScores, GeLU\n",
    "\n",
    "def OrthogonalAttention(num_heads, d_embed, softmax_scale, layer_idx=0):\n",
    "    \"\"\"\n",
    "    Orthogonal attention uses 3-tensors for Q, K, V to make the input and output dimensions explicitly equal.\n",
    "    \"\"\"\n",
    "    Q = TransposeHeads() @ HeadedLinear(num_heads, d_embed, d_embed, tracker=f\"query{layer_idx}\")\n",
    "    K = TransposeHeads() @ HeadedLinear(num_heads, d_embed, d_embed, tracker=f\"key{layer_idx}\")\n",
    "    V = TransposeHeads() @ HeadedLinear(num_heads, d_embed, d_embed, tracker=f\"value{layer_idx}\")\n",
    "    W = HeadedLinearOut(num_heads, d_embed, d_embed, tracker=f\"w{layer_idx}\") @ TransposeHeads()\n",
    "\n",
    "    AttentionScores = Softmax() @ Scalar(tracker=f\"softmax{layer_idx}\") @ CausalMask() @ AttentionQK() @ Rope(d_embed) @ (Q, K)\n",
    "    return ReduceHeads() @ ((1/3) * W) @ ApplyAttentionScores() @ (V, AttentionScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the sensitivity is 1 at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 5 atoms and 11 bonds\n",
      "...smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 5 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "print(OrthogonalAttention(num_heads, d_embed, attention_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual blocks in Modula\n",
    "\n",
    "To implement the rest of our transformer, the roadmap is:\n",
    "* Embed the input tokens\n",
    "* Apply residual blocks for attention and the MLP\n",
    "* Project out\n",
    "\n",
    "All that's left is to set up the residual blocks. In Modula, we define residual connections using a convex combination. If $L$ is the number of residual blocks, then we use a convex combination of the identity and the block to get $x \\mapsto \\frac{L-1}{L} \\cdot x + \\frac{1}{L} \\cdot \\textsf{block}(x)$. The purpose is to create a Lipschitz guarantee that is independent of the number of blocks. For more information, see Proposition 4 of [Scalable Optimization in the Modular Norm](https://arxiv.org/pdf/2405.14813).\n",
    "\n",
    "In short, these changes enable Lipschitz guarantees on our transformer even as we scale the width and the depth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modula.abstract import Identity\n",
    "from modula.atom import Linear, Embed\n",
    "\n",
    "def OrthogonalGPT(vocab_size, num_heads, d_embed, num_blocks, blocks_mass=5):\n",
    "    embed = Embed(d_embed, vocab_size)\n",
    "    embed.tare()\n",
    "\n",
    "    blocks = Identity()\n",
    "    for i in range(num_blocks):\n",
    "        att = OrthogonalAttention(num_heads, d_embed, attention_scale, layer_idx=i)\n",
    "        mlp = Linear(d_embed, d_embed, tracker=f\"mlp_out{i}\") @ GeLU() @ Linear(d_embed, d_embed, tracker=f\"mlp_in{i}\")\n",
    "        att_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * att\n",
    "        mlp_block = (1-1/(2*num_blocks)) * Identity() + 1/(2*num_blocks) * mlp\n",
    "        blocks @= mlp_block @ att_block\n",
    "    \n",
    "    blocks.tare(absolute=blocks_mass)\n",
    "\n",
    "    out = Scalar(tracker=\"final_scale\") @ Linear(vocab_size, d_embed, tracker=\"mlp_final\")\n",
    "\n",
    "    return out @ blocks @ embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we are ready to construct our GPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeModule\n",
      "...consists of 31 atoms and 81 bonds\n",
      "...non-smooth\n",
      "...input sensitivity is 1.0\n",
      "...contributes proportion 8.0 to feature learning of any supermodule\n"
     ]
    }
   ],
   "source": [
    "model = OrthogonalGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    d_embed=d_embed,\n",
    "    num_blocks=num_blocks\n",
    ")\n",
    "\n",
    "# model.jit()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and training\n",
    "\n",
    "To train our transformer we'll use cross entropy loss, which we can compute by decomposing the softmax:\n",
    "\n",
    "$$\n",
    "-\\log(\\text{target probability}) = -\\log(\\text{softmax}(\\text{logits})_\\text{target}) = -\\text{logit}_\\text{target} + \\text{log\\,sum\\,exp}(\\text{logits})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def cross_entropy_loss(w, inputs, targets):\n",
    "    # We use the logsumexp trick for stable cross entropy\n",
    "    logits = model(inputs, w)  # shape is [batch, seq_len, vocab_size]\n",
    "    batch_indices = jnp.arange(logits.shape[0])[:, None]  # shape is [batch, 1]\n",
    "    seq_indices = jnp.arange(logits.shape[1])[None, :]    # shape is [1, seq_len]\n",
    "    # This indexing selects out logits[b, s, targets[b, s]], which is the target logit\n",
    "    losses = -logits[batch_indices, seq_indices, targets] + jax.nn.logsumexp(logits, axis=-1)  # shape is [batch, seq_len]\n",
    "    return losses.mean()\n",
    "\n",
    "# loss_and_grad = jax.jit(jax.value_and_grad(cross_entropy_loss))\n",
    "loss_and_grad = jax.value_and_grad(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 4.228857040405273\n",
      "--> val loss 4.189449310302734\n",
      "Step 10: loss 3.9535036087036133\n",
      "Step 20: loss 3.633457660675049\n",
      "Step 30: loss 3.243577480316162\n",
      "Step 40: loss 2.9053688049316406\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "w = model.initialize(key)\n",
    "log = {}\n",
    "\n",
    "step = 0\n",
    "momentum = [0 * weight for weight in w]\n",
    "lr_schedule = lambda step: lr * (steps - step) / steps\n",
    "for inputs, targets in train_loader:\n",
    "    loss, grad_w = loss_and_grad(w, inputs, targets)\n",
    "    momentum = [beta * m + (1 - beta) * g_w for m, g_w in zip(momentum, grad_w)]\n",
    "    d_w = model.dualize(momentum)\n",
    "    wd_factor = 1 - wd * lr_schedule(step)\n",
    "    w = [wd_factor * weight - lr_schedule(step) * d_weight for weight, d_weight in zip(w, d_w)]\n",
    "    # w = model.project(w)\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: loss {loss}\")\n",
    "        log = model.log(w, grad_w)\n",
    "    \n",
    "    if step % val_interval == 0:\n",
    "        val_losses = []\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            loss, _ = loss_and_grad(w, val_inputs, val_targets)\n",
    "            val_losses.append(loss)\n",
    "            if len(val_losses) >= val_iters:\n",
    "                break\n",
    "        print(f\"--> val loss {sum(val_losses)/len(val_losses)}\")\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    if step >= steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Though this be madness, yet there is method in't\n",
    "\n",
    "And indeed, let us look at how our wee model stacks up to the master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_tokens=100, temperature=0.5, seed=0):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    tokens = jnp.array(encode(prompt))\n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(jnp.expand_dims(tokens, 0), w)\n",
    "        next_token_logits = logits[0, -1] / temperature\n",
    "        \n",
    "        # Sample from our model's token distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        next_token = jax.random.categorical(subkey, next_token_logits)\n",
    "        tokens = jnp.append(tokens, next_token)\n",
    "    \n",
    "    return decode(tokens)\n",
    "\n",
    "for seed in range(3):\n",
    "    print(f\"Sample {seed}:\\n\\n{generate_text('If', max_tokens=100, seed=seed)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical properties\n",
    "\n",
    "We've tracked all sorts of juicy information. Let's dive into a basic measure: how big are the weights during training?\n",
    "\n",
    "To answer this question, let's make a helper function to plot any observable we logged during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot an observable over training: queries (q), keys (k), values (v), out_proj (w), mlp_in, or mlp_out\n",
    "def plot_observable(tracker_start=\"q\", observable=\"weight_norm\"):\n",
    "    trackers = [tracker for tracker in log.keys() if tracker.startswith(tracker_start)]\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.rcParams['axes.linewidth'] = 1.5\n",
    "    plt.rcParams['lines.linewidth'] = 2.5\n",
    "    plt.rcParams['axes.spines.top'] = False\n",
    "    plt.rcParams['axes.spines.right'] = False\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    for tracker in sorted(trackers):\n",
    "        if observable in log[tracker]:\n",
    "            observables = [float(w) if isinstance(w, jnp.ndarray) else w for w in log[tracker][observable]]\n",
    "            steps = jnp.arange(len(observables)) * log_interval\n",
    "            ax.plot(steps, observables, label=f\"{tracker}\", linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel(\"Training steps\", fontsize=16)\n",
    "    ax.set_ylabel(observable, fontsize=16)\n",
    "    ax.set_title(f\"{tracker_start} {observable} over time (weight decay = {wd})\", fontsize=18)\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.2, linewidth=1)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.legend(fontsize=14, framealpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the weight norm of different parts of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [\"q\", \"k\", \"v\", \"w\", \"mlp_in\", \"mlp_out\", \"mlp_final\"]:\n",
    "    # plot_observable(t, \"weight_norm\")\n",
    "    pass\n",
    "    \n",
    "log[\"softmax3\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".modula",
   "language": "python",
   "name": ".modula"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
